2025-12-11 09:53:21,542 DeMo INFO: Saving model in the path :..
2025-12-11 09:53:21,542 DeMo INFO: Namespace(config_file='configs/RGBNT201/DeMo_SDTPS_DGAF_ablation.yml', exp_name=None, fea_cft=0, local_rank=0, opts=['MODEL.USE_SDTPS', 'False', 'MODEL.GLOBAL_LOCAL', 'False', 'MODEL.USE_FRCA', 'False', 'MODEL.USE_DGAF', 'False'])
2025-12-11 09:53:21,542 DeMo INFO: Loaded configuration file configs/RGBNT201/DeMo_SDTPS_DGAF_ablation.yml
2025-12-11 09:53:21,543 DeMo INFO: 
MODEL:
  TRANSFORMER_TYPE: 'ViT-B-16'
  STRIDE_SIZE: [ 16, 16 ]
  SIE_CAMERA: True
  DIRECT: 1
  SIE_COE: 1.0
  ID_LOSS_WEIGHT: 0.25
  TRIPLET_LOSS_WEIGHT: 1.0
  GLOBAL_LOCAL: False  # DGAF V3 直接用 tokens，不需要 GLOBAL_LOCAL
  # Disable HDM and ATM
  HDM: False
  ATM: False
  # SACR disabled for ablation
  USE_SACR: False
  USE_MULTIMODAL_SACR: False
  # LIF disabled
  USE_LIF: False
  # SDTPS configuration
  USE_SDTPS: True
  SDTPS_SPARSE_RATIO: 0.7  # token 保留比例（70%）
  SDTPS_USE_GUMBEL: False  # 禁用 Gumbel（训练稳定性）
  SDTPS_GUMBEL_TAU: 5.0
  SDTPS_CROSS_ATTN_TYPE: 'attention'  # 使用 Cross-Attention（推荐）
  SDTPS_CROSS_ATTN_HEADS: 4  # Cross-Attention 头数
  # 已废弃参数（保留兼容性）
  SDTPS_BETA: 0.25         # 已不使用（原 MLP predictor 权重）
  SDTPS_AGGR_RATIO: 0.5    # 已不使用（原 TokenAggregation）
  SDTPS_LOSS_WEIGHT: 2.0   # SDTPS 分支的损失权重
  # DGAF: Dual-Gated Adaptive Fusion
  USE_DGAF: True
  DGAF_VERSION: 'v3'  # V3 接受 (B,N,C) tokens
  DGAF_TAU: 1.0
  DGAF_INIT_ALPHA: 0.5
  DGAF_NUM_HEADS: 8
  HEAD: 4

INPUT:
  SIZE_TRAIN: [ 256, 128 ]
  SIZE_TEST: [ 256, 128 ]
  PROB: 0.5
  RE_PROB: 0.5
  PADDING: 10

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 8
  NUM_WORKERS: 14

DATASETS:
  NAMES: ('RGBNT201')
  ROOT_DIR: '..'

SOLVER:
  BASE_LR: 0.000005
  LR_SCHEDULER: 'linear'
  MAX_EPOCHS: 50
  STEPS: [30, 40]
  GAMMA: 0.1
  WARMUP_ITERS: 0
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: 'linear'
  OPTIMIZER_NAME: 'Adam'
  IMS_PER_BATCH: 64
  EVAL_PERIOD: 1
  CHECKPOINT_PERIOD: 10

TEST:
  IMS_PER_BATCH: 128
  RE_RANKING: 'no'
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'
  MISS: "nothing"

OUTPUT_DIR: '..'

# ============================================================================
# SDTPS + DGAF Ablation Config
# ============================================================================
# - SACR: disabled
# - SDTPS: configurable via command line
# - DGAF V3: configurable via command line
# - Training: lr=0.00035, epochs=50, warmup=10
# - TF32: enabled in train_net.py
# ============================================================================

2025-12-11 09:53:21,543 DeMo INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 8
  NUM_WORKERS: 14
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: RGBNT201
  ROOT_DIR: ..
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
MODEL:
  ADAPTER: False
  ARCH: DeMo
  ATM: False
  ATT_DROP_RATE: 0.0
  DEVICE: cuda
  DEVICE_ID: 0
  DGAF_INIT_ALPHA: 0.5
  DGAF_LOSS_WEIGHT: 1.0
  DGAF_NUM_HEADS: 8
  DGAF_TAU: 1.0
  DGAF_VERSION: v3
  DIRECT: 1
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FRCA_NEGATIVE_SLOPE: 0.1
  FROZEN: False
  FUSED_LOSS_WEIGHT: 0.5
  GLOBAL_LOCAL: False
  HDM: False
  HEAD: 4
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LIF_BETA: 0.4
  LIF_LAYER: 3
  LIF_LOSS_WEIGHT: 0.1
  METRIC_LOSS_TYPE: triplet
  MULTIMODAL_SACR_VERSION: v1
  NAME: DeMo
  NECK: bnneck
  NO_MARGIN: True
  PRETRAIN_PATH_T: /path/to/your/vitb_16_224_21k.pth
  PROMPT: False
  SACR_DILATION_RATES: [2, 3, 4]
  SDTPS_AGGR_RATIO: 0.5
  SDTPS_BETA: 0.25
  SDTPS_CROSS_ATTN_HEADS: 4
  SDTPS_CROSS_ATTN_TYPE: attention
  SDTPS_GUMBEL_TAU: 5.0
  SDTPS_LOSS_WEIGHT: 2.0
  SDTPS_SHARE_CROSS_ATTN: False
  SDTPS_SPARSE_RATIO: 0.7
  SDTPS_USE_GUMBEL: False
  SIE_CAMERA: True
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: ViT-B-16
  TRIPLET_LOSS_WEIGHT: 1.0
  USE_DGAF: False
  USE_FRCA: False
  USE_LIF: False
  USE_MULTIMODAL_SACR: False
  USE_SACR: False
  USE_SDTPS: False
OUTPUT_DIR: ..
SOLVER:
  BASE_LR: 5e-06
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.1
  IMS_PER_BATCH: 64
  LARGE_FC_LR: False
  LOG_PERIOD: 10
  LR_SCHEDULER: linear
  MARGIN: 0.3
  MAX_EPOCHS: 50
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (30, 40)
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
TEST:
  FEAT: 0
  FEAT_NORM: yes
  IMS_PER_BATCH: 128
  MISS: nothing
  NECK_FEAT: before
  RE_RANKING: no
  WEIGHT: 
=> RGBNT201 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   171 |     3951 |         4
  query    |    30 |      836 |         2
  gallery  |    30 |      836 |         2
  ----------------------------------------
data is ready
using Transformer_type: ViT-B-16 as a backbone
Resized position embedding: %s to %s torch.Size([197, 768]) torch.Size([129, 768])
Position embedding resize to height:16 width: 8
Successfully load ckpt!
<All keys matched successfully>
Loading pretrained model from CLIP
camera number is : 4
===========Building DeMo===========
2025-12-11 09:53:30,930 DeMo INFO: DeMo(
  (BACKBONE): build_transformer(
    (base): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (rgb_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (nir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (tir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (classifier): Linear(in_features=1536, out_features=171, bias=False)
  (bottleneck): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
2025-12-11 09:53:30,931 DeMo INFO: number of parameters:87.988224
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/raid/yij/icme/demo2new/DeMo2/modeling/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
BACKBONE.base.transformer.resblocks.0.attn.out_proj, BACKBONE.base.transformer.resblocks.1.attn.out_proj, BACKBONE.base.transformer.resblocks.10.attn.out_proj, BACKBONE.base.transformer.resblocks.11.attn.out_proj, BACKBONE.base.transformer.resblocks.2.attn.out_proj, BACKBONE.base.transformer.resblocks.3.attn.out_proj, BACKBONE.base.transformer.resblocks.4.attn.out_proj, BACKBONE.base.transformer.resblocks.5.attn.out_proj, BACKBONE.base.transformer.resblocks.6.attn.out_proj, BACKBONE.base.transformer.resblocks.7.attn.out_proj, BACKBONE.base.transformer.resblocks.8.attn.out_proj, BACKBONE.base.transformer.resblocks.9.attn.out_proj, bottleneck, classifier, nir_reduce, nir_reduce.0, nir_reduce.1, nir_reduce.2, pool, rgb_reduce, rgb_reduce.0, rgb_reduce.1, rgb_reduce.2, tir_reduce, tir_reduce.0, tir_reduce.1, tir_reduce.2
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Note: out_proj is calculated in MultiheadAttention.forward(), ignore it
Note: bottleneck/classifier not used in inference, ignore it
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:53:32,484 DeMo INFO: number of GFLOPs: 34.275378708000005
using soft triplet loss for training
label smooth on, numclasses: 171
2025-12-11 09:53:32,500 DeMo INFO: TensorBoard logging to: logs/tensorboard/DeMo_SDTPS_DGAF_ablation_20251211_095332
2025-12-11 09:53:32,500 DeMo INFO: Start TensorBoard with: tensorboard --logdir logs/tensorboard --port 6006
2025-12-11 09:53:32,501 DeMo.train INFO: start training
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-12-11 09:53:36,785 DeMo.train INFO: Epoch[1] Iteration[10/54] Loss: 3.452, Acc: 0.005, Base Lr: 5.00e-06
2025-12-11 09:53:39,138 DeMo.train INFO: Epoch[1] Iteration[20/54] Loss: 2.859, Acc: 0.005, Base Lr: 5.00e-06
2025-12-11 09:53:41,472 DeMo.train INFO: Epoch[1] Iteration[30/54] Loss: 2.616, Acc: 0.008, Base Lr: 5.00e-06
2025-12-11 09:53:43,828 DeMo.train INFO: Epoch[1] Iteration[40/54] Loss: 2.465, Acc: 0.010, Base Lr: 5.00e-06
2025-12-11 09:53:46,173 DeMo.train INFO: Epoch[1] Iteration[50/54] Loss: 2.371, Acc: 0.009, Base Lr: 5.00e-06
2025-12-11 09:53:46,966 DeMo.train INFO: Epoch 1 done. Time per batch: 0.273[s] Speed: 234.5[samples/s]
2025-12-11 09:53:46,967 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:53:46,967 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-11 09:53:46,967 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-11 09:53:51,937 DeMo.train INFO: Validation Results - Epoch: 1
2025-12-11 09:53:51,938 DeMo.train INFO: mAP: 12.5%
2025-12-11 09:53:51,938 DeMo.train INFO: CMC curve, Rank-1  :10.2%
2025-12-11 09:53:51,938 DeMo.train INFO: CMC curve, Rank-5  :19.9%
2025-12-11 09:53:51,938 DeMo.train INFO: CMC curve, Rank-10 :27.4%
2025-12-11 09:53:51,938 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:53:52,421 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:53:52,421 DeMo.train INFO: Best mAP: 12.5%
2025-12-11 09:53:52,421 DeMo.train INFO: Best Rank-1: 10.2%
2025-12-11 09:53:52,421 DeMo.train INFO: Best Rank-5: 19.9%
2025-12-11 09:53:52,422 DeMo.train INFO: Best Rank-10: 27.4%
2025-12-11 09:53:52,422 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:53:55,971 DeMo.train INFO: Epoch[2] Iteration[10/54] Loss: 1.904, Acc: 0.066, Base Lr: 5.00e-06
2025-12-11 09:53:58,328 DeMo.train INFO: Epoch[2] Iteration[20/54] Loss: 1.877, Acc: 0.066, Base Lr: 5.00e-06
2025-12-11 09:54:00,672 DeMo.train INFO: Epoch[2] Iteration[30/54] Loss: 1.868, Acc: 0.062, Base Lr: 5.00e-06
2025-12-11 09:54:03,002 DeMo.train INFO: Epoch[2] Iteration[40/54] Loss: 1.851, Acc: 0.070, Base Lr: 5.00e-06
2025-12-11 09:54:05,335 DeMo.train INFO: Epoch[2] Iteration[50/54] Loss: 1.827, Acc: 0.073, Base Lr: 5.00e-06
2025-12-11 09:54:06,128 DeMo.train INFO: Epoch 2 done. Time per batch: 0.259[s] Speed: 247.5[samples/s]
2025-12-11 09:54:06,130 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:06,130 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-11 09:54:06,130 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-11 09:54:10,747 DeMo.train INFO: Validation Results - Epoch: 2
2025-12-11 09:54:10,747 DeMo.train INFO: mAP: 40.2%
2025-12-11 09:54:10,748 DeMo.train INFO: CMC curve, Rank-1  :40.0%
2025-12-11 09:54:10,748 DeMo.train INFO: CMC curve, Rank-5  :56.9%
2025-12-11 09:54:10,748 DeMo.train INFO: CMC curve, Rank-10 :63.9%
2025-12-11 09:54:10,748 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:11,264 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:11,264 DeMo.train INFO: Best mAP: 40.2%
2025-12-11 09:54:11,264 DeMo.train INFO: Best Rank-1: 40.0%
2025-12-11 09:54:11,264 DeMo.train INFO: Best Rank-5: 56.9%
2025-12-11 09:54:11,264 DeMo.train INFO: Best Rank-10: 63.9%
2025-12-11 09:54:11,265 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:14,612 DeMo.train INFO: Epoch[3] Iteration[10/54] Loss: 1.646, Acc: 0.213, Base Lr: 5.00e-06
2025-12-11 09:54:16,988 DeMo.train INFO: Epoch[3] Iteration[20/54] Loss: 1.660, Acc: 0.232, Base Lr: 5.00e-06
2025-12-11 09:54:19,337 DeMo.train INFO: Epoch[3] Iteration[30/54] Loss: 1.634, Acc: 0.223, Base Lr: 5.00e-06
2025-12-11 09:54:21,679 DeMo.train INFO: Epoch[3] Iteration[40/54] Loss: 1.622, Acc: 0.213, Base Lr: 5.00e-06
2025-12-11 09:54:23,998 DeMo.train INFO: Epoch[3] Iteration[50/54] Loss: 1.634, Acc: 0.210, Base Lr: 5.00e-06
2025-12-11 09:54:24,798 DeMo.train INFO: Epoch 3 done. Time per batch: 0.255[s] Speed: 250.6[samples/s]
2025-12-11 09:54:24,799 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:24,799 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-11 09:54:24,799 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-11 09:54:29,337 DeMo.train INFO: Validation Results - Epoch: 3
2025-12-11 09:54:29,337 DeMo.train INFO: mAP: 48.7%
2025-12-11 09:54:29,337 DeMo.train INFO: CMC curve, Rank-1  :48.2%
2025-12-11 09:54:29,337 DeMo.train INFO: CMC curve, Rank-5  :63.5%
2025-12-11 09:54:29,337 DeMo.train INFO: CMC curve, Rank-10 :71.7%
2025-12-11 09:54:29,337 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:29,857 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:29,857 DeMo.train INFO: Best mAP: 48.7%
2025-12-11 09:54:29,857 DeMo.train INFO: Best Rank-1: 48.2%
2025-12-11 09:54:29,858 DeMo.train INFO: Best Rank-5: 63.5%
2025-12-11 09:54:29,858 DeMo.train INFO: Best Rank-10: 71.7%
2025-12-11 09:54:29,858 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:33,335 DeMo.train INFO: Epoch[4] Iteration[10/54] Loss: 1.520, Acc: 0.347, Base Lr: 5.00e-06
2025-12-11 09:54:35,713 DeMo.train INFO: Epoch[4] Iteration[20/54] Loss: 1.486, Acc: 0.391, Base Lr: 5.00e-06
2025-12-11 09:54:38,086 DeMo.train INFO: Epoch[4] Iteration[30/54] Loss: 1.495, Acc: 0.385, Base Lr: 5.00e-06
2025-12-11 09:54:40,441 DeMo.train INFO: Epoch[4] Iteration[40/54] Loss: 1.485, Acc: 0.388, Base Lr: 5.00e-06
2025-12-11 09:54:42,777 DeMo.train INFO: Epoch[4] Iteration[50/54] Loss: 1.477, Acc: 0.419, Base Lr: 5.00e-06
2025-12-11 09:54:43,585 DeMo.train INFO: Epoch 4 done. Time per batch: 0.259[s] Speed: 247.1[samples/s]
2025-12-11 09:54:43,586 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-11 09:54:43,586 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-11 09:54:43,586 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
