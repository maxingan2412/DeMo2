2025-12-09 12:28:45,766 DeMo INFO: Saving model in the path :..
2025-12-09 12:28:45,766 DeMo INFO: Namespace(config_file='configs/RGBNT201/DeMo_SDTPS_DGAF_ablation.yml', exp_name=None, fea_cft=0, local_rank=0, opts=['MODEL.USE_SDTPS', 'False', 'MODEL.USE_DGAF', 'False', 'MODEL.GLOBAL_LOCAL', 'False'])
2025-12-09 12:28:45,766 DeMo INFO: Loaded configuration file configs/RGBNT201/DeMo_SDTPS_DGAF_ablation.yml
2025-12-09 12:28:45,766 DeMo INFO: 
MODEL:
  TRANSFORMER_TYPE: 'ViT-B-16'
  STRIDE_SIZE: [ 16, 16 ]
  SIE_CAMERA: True
  DIRECT: 1
  SIE_COE: 1.0
  ID_LOSS_WEIGHT: 0.25
  TRIPLET_LOSS_WEIGHT: 1.0
  GLOBAL_LOCAL: False  # DGAF V3 直接用 tokens，不需要 GLOBAL_LOCAL
  # Disable HDM and ATM
  HDM: False
  ATM: False
  # SACR disabled for ablation
  USE_SACR: False
  USE_MULTIMODAL_SACR: False
  # LIF disabled
  USE_LIF: False
  # SDTPS configuration
  USE_SDTPS: True
  SDTPS_SPARSE_RATIO: 0.7  # token 保留比例（70%）
  SDTPS_USE_GUMBEL: False  # 禁用 Gumbel（训练稳定性）
  SDTPS_GUMBEL_TAU: 5.0
  SDTPS_CROSS_ATTN_TYPE: 'attention'  # 使用 Cross-Attention（推荐）
  SDTPS_CROSS_ATTN_HEADS: 4  # Cross-Attention 头数
  # 已废弃参数（保留兼容性）
  SDTPS_BETA: 0.25         # 已不使用（原 MLP predictor 权重）
  SDTPS_AGGR_RATIO: 0.5    # 已不使用（原 TokenAggregation）
  SDTPS_LOSS_WEIGHT: 2.0   # SDTPS 分支的损失权重
  # DGAF: Dual-Gated Adaptive Fusion
  USE_DGAF: True
  DGAF_VERSION: 'v3'  # V3 接受 (B,N,C) tokens
  DGAF_TAU: 1.0
  DGAF_INIT_ALPHA: 0.5
  DGAF_NUM_HEADS: 8
  HEAD: 4

INPUT:
  SIZE_TRAIN: [ 256, 128 ]
  SIZE_TEST: [ 256, 128 ]
  PROB: 0.5
  RE_PROB: 0.5
  PADDING: 10

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 8
  NUM_WORKERS: 14

DATASETS:
  NAMES: ('RGBNT201')
  ROOT_DIR: '..'

SOLVER:
  BASE_LR: 0.000005
  LR_SCHEDULER: 'linear'
  MAX_EPOCHS: 50
  STEPS: [30, 40]
  GAMMA: 0.1
  WARMUP_ITERS: 0
  WARMUP_FACTOR: 0.01
  WARMUP_METHOD: 'linear'
  OPTIMIZER_NAME: 'Adam'
  IMS_PER_BATCH: 64
  EVAL_PERIOD: 1
  CHECKPOINT_PERIOD: 10

TEST:
  IMS_PER_BATCH: 128
  RE_RANKING: 'no'
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'
  MISS: "nothing"

OUTPUT_DIR: '..'

# ============================================================================
# SDTPS + DGAF Ablation Config
# ============================================================================
# - SACR: disabled
# - SDTPS: configurable via command line
# - DGAF V3: configurable via command line
# - Training: lr=0.00035, epochs=50, warmup=10
# - TF32: enabled in train_net.py
# ============================================================================

2025-12-09 12:28:45,767 DeMo INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 8
  NUM_WORKERS: 14
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: RGBNT201
  ROOT_DIR: ..
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
MODEL:
  ADAPTER: False
  ATM: False
  ATT_DROP_RATE: 0.0
  DEVICE: cuda
  DEVICE_ID: 0
  DGAF_INIT_ALPHA: 0.5
  DGAF_NUM_HEADS: 8
  DGAF_TAU: 1.0
  DGAF_VERSION: v3
  DIRECT: 1
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FROZEN: False
  GLOBAL_LOCAL: False
  HDM: False
  HEAD: 4
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LIF_BETA: 0.4
  LIF_LAYER: 3
  LIF_LOSS_WEIGHT: 0.1
  METRIC_LOSS_TYPE: triplet
  MULTIMODAL_SACR_VERSION: v1
  NAME: DeMo
  NECK: bnneck
  NO_MARGIN: True
  PRETRAIN_PATH_T: /path/to/your/vitb_16_224_21k.pth
  PROMPT: False
  SACR_DILATION_RATES: [2, 3, 4]
  SDTPS_AGGR_RATIO: 0.5
  SDTPS_BETA: 0.25
  SDTPS_CROSS_ATTN_HEADS: 4
  SDTPS_CROSS_ATTN_TYPE: attention
  SDTPS_GUMBEL_TAU: 5.0
  SDTPS_LOSS_WEIGHT: 2.0
  SDTPS_SPARSE_RATIO: 0.7
  SDTPS_USE_GUMBEL: False
  SIE_CAMERA: True
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: ViT-B-16
  TRIPLET_LOSS_WEIGHT: 1.0
  USE_DGAF: False
  USE_LIF: False
  USE_MULTIMODAL_SACR: False
  USE_SACR: False
  USE_SDTPS: False
OUTPUT_DIR: ..
SOLVER:
  BASE_LR: 5e-06
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 1
  GAMMA: 0.1
  IMS_PER_BATCH: 64
  LARGE_FC_LR: False
  LOG_PERIOD: 10
  LR_SCHEDULER: linear
  MARGIN: 0.3
  MAX_EPOCHS: 50
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (30, 40)
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
TEST:
  FEAT: 0
  FEAT_NORM: yes
  IMS_PER_BATCH: 128
  MISS: nothing
  NECK_FEAT: before
  RE_RANKING: no
  WEIGHT: 
=> RGBNT201 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   171 |     3951 |         4
  query    |    30 |      836 |         2
  gallery  |    30 |      836 |         2
  ----------------------------------------
data is ready
using Transformer_type: ViT-B-16 as a backbone
Resized position embedding: %s to %s torch.Size([197, 768]) torch.Size([129, 768])
Position embedding resize to height:16 width: 8
Successfully load ckpt!
<All keys matched successfully>
Loading pretrained model from CLIP
camera number is : 4
===========Building DeMo===========
2025-12-09 12:28:56,584 DeMo INFO: DeMo(
  (BACKBONE): build_transformer(
    (base): VisionTransformer(
      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (pool): AdaptiveAvgPool1d(output_size=1)
  (rgb_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (nir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (tir_reduce): Sequential(
    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=1024, out_features=512, bias=True)
    (2): QuickGELU()
  )
  (classifier): Linear(in_features=1536, out_features=171, bias=False)
  (bottleneck): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
2025-12-09 12:28:56,585 DeMo INFO: number of parameters:87.988224
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/raid/yij/icme/demo2new/DeMo2/modeling/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
BACKBONE.base.transformer.resblocks.0.attn.out_proj, BACKBONE.base.transformer.resblocks.1.attn.out_proj, BACKBONE.base.transformer.resblocks.10.attn.out_proj, BACKBONE.base.transformer.resblocks.11.attn.out_proj, BACKBONE.base.transformer.resblocks.2.attn.out_proj, BACKBONE.base.transformer.resblocks.3.attn.out_proj, BACKBONE.base.transformer.resblocks.4.attn.out_proj, BACKBONE.base.transformer.resblocks.5.attn.out_proj, BACKBONE.base.transformer.resblocks.6.attn.out_proj, BACKBONE.base.transformer.resblocks.7.attn.out_proj, BACKBONE.base.transformer.resblocks.8.attn.out_proj, BACKBONE.base.transformer.resblocks.9.attn.out_proj, bottleneck, classifier, nir_reduce, nir_reduce.0, nir_reduce.1, nir_reduce.2, pool, rgb_reduce, rgb_reduce.0, rgb_reduce.1, rgb_reduce.2, tir_reduce, tir_reduce.0, tir_reduce.1, tir_reduce.2
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The out_proj here is called by the nn.MultiheadAttention, which has been calculated in th .forward(), so just ignore it.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
For the bottleneck or classifier, it is not calculated during inference, so just ignore it.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:28:59,520 DeMo INFO: number of GFLOPs: 34.275378708000005
using soft triplet loss for training
label smooth on, numclasses: 171
2025-12-09 12:28:59,532 DeMo.train INFO: start training
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-12-09 12:29:05,118 DeMo.train INFO: Epoch[1] Iteration[10/54] Loss: 3.436, Acc: 0.003, Base Lr: 5.00e-06
2025-12-09 12:29:07,469 DeMo.train INFO: Epoch[1] Iteration[20/54] Loss: 2.875, Acc: 0.007, Base Lr: 5.00e-06
2025-12-09 12:29:09,803 DeMo.train INFO: Epoch[1] Iteration[30/54] Loss: 2.647, Acc: 0.006, Base Lr: 5.00e-06
2025-12-09 12:29:12,133 DeMo.train INFO: Epoch[1] Iteration[40/54] Loss: 2.500, Acc: 0.009, Base Lr: 5.00e-06
2025-12-09 12:29:14,466 DeMo.train INFO: Epoch[1] Iteration[50/54] Loss: 2.403, Acc: 0.010, Base Lr: 5.00e-06
2025-12-09 12:29:15,305 DeMo.train INFO: Epoch 1 done. Time per batch: 0.297[s] Speed: 215.2[samples/s]
2025-12-09 12:29:15,305 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:29:15,306 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-09 12:29:15,306 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-09 12:29:21,680 DeMo.train INFO: Validation Results - Epoch: 1
2025-12-09 12:29:21,681 DeMo.train INFO: mAP: 11.6%
2025-12-09 12:29:21,681 DeMo.train INFO: CMC curve, Rank-1  :8.6%
2025-12-09 12:29:21,681 DeMo.train INFO: CMC curve, Rank-5  :16.6%
2025-12-09 12:29:21,681 DeMo.train INFO: CMC curve, Rank-10 :25.0%
2025-12-09 12:29:21,681 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:29:50,801 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:29:50,804 DeMo.train INFO: Best mAP: 11.6%
2025-12-09 12:29:50,805 DeMo.train INFO: Best Rank-1: 8.6%
2025-12-09 12:29:50,806 DeMo.train INFO: Best Rank-5: 16.6%
2025-12-09 12:29:50,808 DeMo.train INFO: Best Rank-10: 25.0%
2025-12-09 12:29:50,809 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:29:58,112 DeMo.train INFO: Epoch[2] Iteration[10/54] Loss: 1.926, Acc: 0.044, Base Lr: 4.90e-06
2025-12-09 12:30:00,469 DeMo.train INFO: Epoch[2] Iteration[20/54] Loss: 1.894, Acc: 0.045, Base Lr: 4.90e-06
2025-12-09 12:30:02,810 DeMo.train INFO: Epoch[2] Iteration[30/54] Loss: 1.878, Acc: 0.057, Base Lr: 4.90e-06
2025-12-09 12:30:05,147 DeMo.train INFO: Epoch[2] Iteration[40/54] Loss: 1.859, Acc: 0.062, Base Lr: 4.90e-06
2025-12-09 12:30:07,474 DeMo.train INFO: Epoch[2] Iteration[50/54] Loss: 1.834, Acc: 0.070, Base Lr: 4.90e-06
2025-12-09 12:30:08,304 DeMo.train INFO: Epoch 2 done. Time per batch: 0.330[s] Speed: 193.9[samples/s]
2025-12-09 12:30:08,305 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:30:08,305 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-09 12:30:08,305 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-09 12:30:14,224 DeMo.train INFO: Validation Results - Epoch: 2
2025-12-09 12:30:14,224 DeMo.train INFO: mAP: 31.3%
2025-12-09 12:30:14,224 DeMo.train INFO: CMC curve, Rank-1  :28.1%
2025-12-09 12:30:14,224 DeMo.train INFO: CMC curve, Rank-5  :44.1%
2025-12-09 12:30:14,224 DeMo.train INFO: CMC curve, Rank-10 :54.1%
2025-12-09 12:30:14,224 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:30:45,071 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:30:45,072 DeMo.train INFO: Best mAP: 31.3%
2025-12-09 12:30:45,073 DeMo.train INFO: Best Rank-1: 28.1%
2025-12-09 12:30:45,074 DeMo.train INFO: Best Rank-5: 44.1%
2025-12-09 12:30:45,074 DeMo.train INFO: Best Rank-10: 54.1%
2025-12-09 12:30:45,075 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:30:50,290 DeMo.train INFO: Epoch[3] Iteration[10/54] Loss: 1.632, Acc: 0.173, Base Lr: 4.80e-06
2025-12-09 12:30:52,651 DeMo.train INFO: Epoch[3] Iteration[20/54] Loss: 1.645, Acc: 0.192, Base Lr: 4.80e-06
2025-12-09 12:30:54,998 DeMo.train INFO: Epoch[3] Iteration[30/54] Loss: 1.620, Acc: 0.209, Base Lr: 4.80e-06
2025-12-09 12:30:57,354 DeMo.train INFO: Epoch[3] Iteration[40/54] Loss: 1.609, Acc: 0.207, Base Lr: 4.80e-06
2025-12-09 12:30:59,706 DeMo.train INFO: Epoch[3] Iteration[50/54] Loss: 1.590, Acc: 0.214, Base Lr: 4.80e-06
2025-12-09 12:31:00,554 DeMo.train INFO: Epoch 3 done. Time per batch: 0.292[s] Speed: 219.2[samples/s]
2025-12-09 12:31:00,555 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:31:00,555 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-09 12:31:00,555 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-09 12:31:06,513 DeMo.train INFO: Validation Results - Epoch: 3
2025-12-09 12:31:06,513 DeMo.train INFO: mAP: 50.2%
2025-12-09 12:31:06,513 DeMo.train INFO: CMC curve, Rank-1  :50.8%
2025-12-09 12:31:06,513 DeMo.train INFO: CMC curve, Rank-5  :65.6%
2025-12-09 12:31:06,513 DeMo.train INFO: CMC curve, Rank-10 :73.3%
2025-12-09 12:31:06,513 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:31:36,487 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:31:36,488 DeMo.train INFO: Best mAP: 50.2%
2025-12-09 12:31:36,490 DeMo.train INFO: Best Rank-1: 50.8%
2025-12-09 12:31:36,490 DeMo.train INFO: Best Rank-5: 65.6%
2025-12-09 12:31:36,491 DeMo.train INFO: Best Rank-10: 73.3%
2025-12-09 12:31:36,491 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:31:42,243 DeMo.train INFO: Epoch[4] Iteration[10/54] Loss: 1.552, Acc: 0.347, Base Lr: 4.69e-06
2025-12-09 12:31:44,598 DeMo.train INFO: Epoch[4] Iteration[20/54] Loss: 1.492, Acc: 0.425, Base Lr: 4.69e-06
2025-12-09 12:31:46,926 DeMo.train INFO: Epoch[4] Iteration[30/54] Loss: 1.484, Acc: 0.425, Base Lr: 4.69e-06
2025-12-09 12:31:49,266 DeMo.train INFO: Epoch[4] Iteration[40/54] Loss: 1.469, Acc: 0.416, Base Lr: 4.69e-06
2025-12-09 12:31:51,608 DeMo.train INFO: Epoch[4] Iteration[50/54] Loss: 1.453, Acc: 0.432, Base Lr: 4.69e-06
2025-12-09 12:31:52,445 DeMo.train INFO: Epoch 4 done. Time per batch: 0.301[s] Speed: 212.6[samples/s]
2025-12-09 12:31:52,447 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:31:52,447 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-09 12:31:52,447 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-09 12:31:58,289 DeMo.train INFO: Validation Results - Epoch: 4
2025-12-09 12:31:58,289 DeMo.train INFO: mAP: 55.8%
2025-12-09 12:31:58,289 DeMo.train INFO: CMC curve, Rank-1  :56.5%
2025-12-09 12:31:58,290 DeMo.train INFO: CMC curve, Rank-5  :68.3%
2025-12-09 12:31:58,290 DeMo.train INFO: CMC curve, Rank-10 :74.2%
2025-12-09 12:31:58,290 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:32:27,554 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:32:27,555 DeMo.train INFO: Best mAP: 55.8%
2025-12-09 12:32:27,556 DeMo.train INFO: Best Rank-1: 56.5%
2025-12-09 12:32:27,557 DeMo.train INFO: Best Rank-5: 68.3%
2025-12-09 12:32:27,557 DeMo.train INFO: Best Rank-10: 74.2%
2025-12-09 12:32:27,557 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:32:34,185 DeMo.train INFO: Epoch[5] Iteration[10/54] Loss: 1.403, Acc: 0.500, Base Lr: 4.59e-06
2025-12-09 12:32:37,071 DeMo.train INFO: Epoch[5] Iteration[20/54] Loss: 1.419, Acc: 0.530, Base Lr: 4.59e-06
2025-12-09 12:32:39,406 DeMo.train INFO: Epoch[5] Iteration[30/54] Loss: 1.413, Acc: 0.588, Base Lr: 4.59e-06
2025-12-09 12:32:41,737 DeMo.train INFO: Epoch[5] Iteration[40/54] Loss: 1.398, Acc: 0.602, Base Lr: 4.59e-06
2025-12-09 12:32:44,073 DeMo.train INFO: Epoch[5] Iteration[50/54] Loss: 1.386, Acc: 0.622, Base Lr: 4.59e-06
2025-12-09 12:32:44,919 DeMo.train INFO: Epoch 5 done. Time per batch: 0.328[s] Speed: 195.4[samples/s]
2025-12-09 12:32:44,921 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:32:44,921 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-09 12:32:44,921 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The test feature is normalized
=> Computing DistMat with euclidean_distance
2025-12-09 12:32:50,616 DeMo.train INFO: Validation Results - Epoch: 5
2025-12-09 12:32:50,616 DeMo.train INFO: mAP: 57.3%
2025-12-09 12:32:50,616 DeMo.train INFO: CMC curve, Rank-1  :56.6%
2025-12-09 12:32:50,617 DeMo.train INFO: CMC curve, Rank-5  :70.8%
2025-12-09 12:32:50,617 DeMo.train INFO: CMC curve, Rank-10 :77.4%
2025-12-09 12:32:50,617 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:33:15,465 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:33:15,466 DeMo.train INFO: Best mAP: 57.3%
2025-12-09 12:33:15,468 DeMo.train INFO: Best Rank-1: 56.6%
2025-12-09 12:33:15,468 DeMo.train INFO: Best Rank-5: 70.8%
2025-12-09 12:33:15,471 DeMo.train INFO: Best Rank-10: 77.4%
2025-12-09 12:33:15,471 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:33:26,806 DeMo.train INFO: Epoch[6] Iteration[10/54] Loss: 1.333, Acc: 0.700, Base Lr: 4.49e-06
2025-12-09 12:33:29,144 DeMo.train INFO: Epoch[6] Iteration[20/54] Loss: 1.335, Acc: 0.700, Base Lr: 4.49e-06
2025-12-09 12:33:31,471 DeMo.train INFO: Epoch[6] Iteration[30/54] Loss: 1.332, Acc: 0.707, Base Lr: 4.49e-06
2025-12-09 12:33:33,810 DeMo.train INFO: Epoch[6] Iteration[40/54] Loss: 1.328, Acc: 0.725, Base Lr: 4.49e-06
2025-12-09 12:33:36,144 DeMo.train INFO: Epoch[6] Iteration[50/54] Loss: 1.331, Acc: 0.733, Base Lr: 4.49e-06
2025-12-09 12:33:36,977 DeMo.train INFO: Epoch 6 done. Time per batch: 0.406[s] Speed: 157.7[samples/s]
2025-12-09 12:33:36,978 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2025-12-09 12:33:36,978 DeMo.train INFO: Current is the [moe,ori] feature testing!
2025-12-09 12:33:36,978 DeMo.train INFO: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
