2025-12-10 16:17:00,077 DeMo INFO: Saving model in the path :..
2025-12-10 16:17:00,077 DeMo INFO: Namespace(config_file='configs/RGBNT201/DeMo_Parallel.yml', exp_name=None, fea_cft=0, local_rank=0, opts=['MODEL.SDTPS_LOSS_WEIGHT', '1.0', 'MODEL.DGAF_LOSS_WEIGHT', '0.0', 'MODEL.FUSED_LOSS_WEIGHT', '1.0'])
2025-12-10 16:17:00,077 DeMo INFO: Loaded configuration file configs/RGBNT201/DeMo_Parallel.yml
2025-12-10 16:17:00,078 DeMo INFO: 
MODEL:
  ARCH: 'DeMo_Parallel'  # 使用并行架构（9个分类头）
  TRANSFORMER_TYPE: 'ViT-B-16'
  STRIDE_SIZE: [ 16, 16 ]
  SIE_CAMERA: True
  DIRECT: 1
  SIE_COE: 1.0
  ID_LOSS_WEIGHT: 0.25
  TRIPLET_LOSS_WEIGHT: 1.0

  # Global-Local Fusion: 启用（提升特征表达能力）
  GLOBAL_LOCAL: True

  # 禁用旧模块（HDM/ATM）
  HDM: False
  ATM: False
  USE_SACR: False
  USE_MULTIMODAL_SACR: False
  USE_LIF: False

  # ========== 并行架构核心参数 ==========
  # SDTPS 配置（分支1）
  USE_SDTPS: True
  SDTPS_SPARSE_RATIO: 0.6  # token 保留比例（60%）
  SDTPS_USE_GUMBEL: False  # 禁用 Gumbel（训练稳定性）
  SDTPS_GUMBEL_TAU: 5.0
  SDTPS_CROSS_ATTN_TYPE: 'attention'  # Cross-Attention
  SDTPS_CROSS_ATTN_HEADS: 4
  SDTPS_BETA: 0.25         # 保留兼容性
  SDTPS_AGGR_RATIO: 0.5    # 保留兼容性
  SDTPS_LOSS_WEIGHT: 1.0   # SDTPS 分支权重（建议1.0）

  # DGAF 配置（分支2）
  USE_DGAF: True
  DGAF_VERSION: 'v4'  # V4: 返回3个独立特征
  DGAF_TAU: 1.0  # 信息熵门控温度
  DGAF_INIT_ALPHA: 0.5  # IEG 和 MIG 的平衡系数
  DGAF_LOSS_WEIGHT: 1.0  # DGAF 分支权重（建议1.0）

  # Fused 配置（分支3，辅助监督）
  FUSED_LOSS_WEIGHT: 0.5  # Fused 分支权重（建议0.5，辅助作用）

  # ========== 过拟合缓解 ==========
  DROP_OUT: 0.0  # 使用defaults.py中已有的参数
  IF_LABELSMOOTH: 'on'  # Label Smoothing (注意: 必须是字符串 'on' 或 'off')

  HEAD: 4  # 保留兼容性

INPUT:
  SIZE_TRAIN: [ 256, 128 ]
  SIZE_TEST: [ 256, 128 ]
  PROB: 0.5  # random horizontal flip
  RE_PROB: 0.5  # random erasing（数据增强）
  PADDING: 10

DATALOADER:
  SAMPLER: 'softmax_triplet'
  NUM_INSTANCE: 4  # 减小以节省显存（原8）
  NUM_WORKERS: 14

DATASETS:
  NAMES: ('RGBNT201')
  ROOT_DIR: '..'

SOLVER:
  BASE_LR: 0.0003  # 初始学习率（可根据需要调整）
  WARMUP_ITERS: 5  # Warmup epochs
  MAX_EPOCHS: 60  # 增加训练轮数（9个分类头需要更多时间）
  OPTIMIZER_NAME: 'Adam'
  IMS_PER_BATCH: 48  # 减小 batch size（9个分类头占用更多显存）
  EVAL_PERIOD: 15
  WEIGHT_DECAY: 0.0005  # 增大正则化

  # ========== 分支权重（在 processor.py 中使用）==========
  # 根据分支索引应用不同权重：
  # - SDTPS (i=0,2,4): SDTPS_LOSS_WEIGHT
  # - DGAF (i=6,8,10): DGAF_LOSS_WEIGHT
  # - Fused (i=12,14,16): FUSED_LOSS_WEIGHT

TEST:
  IMS_PER_BATCH: 128
  RE_RANKING: 'no'
  WEIGHT: ''
  NECK_FEAT: 'before'
  FEAT_NORM: 'yes'
  MISS: "nothing"  # 缺失模态测试：'r', 'n', 't', 'rn', 'rt', 'nt', 'nothing'

OUTPUT_DIR: '..'

2025-12-10 16:17:00,078 DeMo INFO: Running with config:
DATALOADER:
  NUM_INSTANCE: 4
  NUM_WORKERS: 14
  SAMPLER: softmax_triplet
DATASETS:
  NAMES: RGBNT201
  ROOT_DIR: ..
INPUT:
  PADDING: 10
  PIXEL_MEAN: [0.5, 0.5, 0.5]
  PIXEL_STD: [0.5, 0.5, 0.5]
  PROB: 0.5
  RE_PROB: 0.5
  SIZE_TEST: [256, 128]
  SIZE_TRAIN: [256, 128]
MODEL:
  ADAPTER: False
  ARCH: DeMo_Parallel
  ATM: False
  ATT_DROP_RATE: 0.0
  DEVICE: cuda
  DEVICE_ID: 0
  DGAF_INIT_ALPHA: 0.5
  DGAF_LOSS_WEIGHT: 0.0
  DGAF_NUM_HEADS: 8
  DGAF_TAU: 1.0
  DGAF_VERSION: v4
  DIRECT: 1
  DIST_TRAIN: False
  DROP_OUT: 0.0
  DROP_PATH: 0.1
  FROZEN: False
  FUSED_LOSS_WEIGHT: 1.0
  GLOBAL_LOCAL: True
  HDM: False
  HEAD: 4
  ID_LOSS_TYPE: softmax
  ID_LOSS_WEIGHT: 0.25
  IF_LABELSMOOTH: on
  IF_WITH_CENTER: no
  LIF_BETA: 0.4
  LIF_LAYER: 3
  LIF_LOSS_WEIGHT: 0.1
  METRIC_LOSS_TYPE: triplet
  MULTIMODAL_SACR_VERSION: v1
  NAME: DeMo
  NECK: bnneck
  NO_MARGIN: True
  PRETRAIN_PATH_T: /path/to/your/vitb_16_224_21k.pth
  PROMPT: False
  SACR_DILATION_RATES: [2, 3, 4]
  SDTPS_AGGR_RATIO: 0.5
  SDTPS_BETA: 0.25
  SDTPS_CROSS_ATTN_HEADS: 4
  SDTPS_CROSS_ATTN_TYPE: attention
  SDTPS_GUMBEL_TAU: 5.0
  SDTPS_LOSS_WEIGHT: 1.0
  SDTPS_SHARE_CROSS_ATTN: False
  SDTPS_SPARSE_RATIO: 0.6
  SDTPS_USE_GUMBEL: False
  SIE_CAMERA: True
  SIE_COE: 1.0
  SIE_VIEW: False
  STRIDE_SIZE: [16, 16]
  TRANSFORMER_TYPE: ViT-B-16
  TRIPLET_LOSS_WEIGHT: 1.0
  USE_DGAF: True
  USE_LIF: False
  USE_MULTIMODAL_SACR: False
  USE_SACR: False
  USE_SDTPS: True
OUTPUT_DIR: ..
SOLVER:
  BASE_LR: 0.0003
  CENTER_LOSS_WEIGHT: 0.0005
  CENTER_LR: 0.5
  CHECKPOINT_PERIOD: 10
  CLUSTER_MARGIN: 0.3
  COSINE_MARGIN: 0.5
  COSINE_SCALE: 30
  EVAL_PERIOD: 15
  GAMMA: 0.1
  IMS_PER_BATCH: 48
  LARGE_FC_LR: False
  LOG_PERIOD: 10
  LR_SCHEDULER: cosine
  MARGIN: 0.3
  MAX_EPOCHS: 60
  MOMENTUM: 0.9
  OPTIMIZER_NAME: Adam
  RANGE_ALPHA: 0
  RANGE_BETA: 1
  RANGE_K: 2
  RANGE_LOSS_WEIGHT: 1
  RANGE_MARGIN: 0.3
  SEED: 1234
  STEPS: (40, 70)
  WARMUP_FACTOR: 0.01
  WARMUP_ITERS: 5
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0005
  WEIGHT_DECAY_BIAS: 0.0001
TEST:
  FEAT: 0
  FEAT_NORM: yes
  IMS_PER_BATCH: 128
  MISS: nothing
  NECK_FEAT: before
  RE_RANKING: no
  WEIGHT: 
=> RGBNT201 loaded
Dataset statistics:
  ----------------------------------------
  subset   | # ids | # images | # cameras
  ----------------------------------------
  train    |   171 |     3951 |         4
  query    |    30 |      836 |         2
  gallery  |    30 |      836 |         2
  ----------------------------------------
data is ready
using Transformer_type: ViT-B-16 as a backbone
Resized position embedding: %s to %s torch.Size([197, 768]) torch.Size([129, 768])
Position embedding resize to height:16 width: 8
Successfully load ckpt!
<All keys matched successfully>
Loading pretrained model from CLIP
camera number is : 4
===========Building DeMo_Parallel (9 heads)===========
model has no flops
using soft triplet loss for training
label smooth on, numclasses: 171
2025-12-10 16:17:08,360 DeMo INFO: TensorBoard logging to: logs/tensorboard/DeMo_Parallel_20251210_161708
2025-12-10 16:17:08,360 DeMo INFO: Start TensorBoard with: tensorboard --logdir logs/tensorboard --port 6006
2025-12-10 16:17:08,360 DeMo.train INFO: start training
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.models", FutureWarning)
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/raid/yij/icme/demo2new/DeMo2/modeling/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended
  warnings.warn("PyTorch version 1.7.1 or higher is recommended")
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/yij/miniconda3/envs/DeMo/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-12-10 16:17:16,106 DeMo.train INFO: Epoch[1] Iteration[10/78] Loss: 25.062, Acc: 0.000, Base Lr: 6.24e-05
2025-12-10 16:17:18,433 DeMo.train INFO: Epoch[1] Iteration[20/78] Loss: 22.013, Acc: 0.006, Base Lr: 6.24e-05
2025-12-10 16:17:20,753 DeMo.train INFO: Epoch[1] Iteration[30/78] Loss: 20.810, Acc: 0.013, Base Lr: 6.24e-05
2025-12-10 16:17:23,072 DeMo.train INFO: Epoch[1] Iteration[40/78] Loss: 20.088, Acc: 0.022, Base Lr: 6.24e-05
2025-12-10 16:17:25,382 DeMo.train INFO: Epoch[1] Iteration[50/78] Loss: 19.649, Acc: 0.035, Base Lr: 6.24e-05
2025-12-10 16:17:27,704 DeMo.train INFO: Epoch[1] Iteration[60/78] Loss: 19.291, Acc: 0.040, Base Lr: 6.24e-05
