# SDTPS å®Œæ•´å®ç°æ€»ç»“ - æœ€ç»ˆç‰ˆæœ¬

## âœ… å®Œæˆçš„æ‰€æœ‰ä¿®å¤

### ä¿®å¤ 1ï¼šæ·»åŠ  TokenAggregationï¼ˆâœ… æœ€å…³é”®ï¼‰

**é—®é¢˜**ï¼šä¹‹å‰å®Œå…¨ç¼ºå¤±è¿™ä¸€æ­¥ï¼Œå¯¼è‡´è¾“å‡ºè¿‡å¤špatches
**ä¿®å¤**ï¼šä»åŸè®ºæ–‡å’ŒåŸä»£ç æå–å®Œæ•´çš„TokenAggregationæ¨¡å—

```python
class TokenAggregation(nn.Module):
    """
    å¯¹åº”è®ºæ–‡å…¬å¼4: vÌ‚_j = Î£_i W_{ij} Â· v_i
    """
    def __init__(self, dim=512, keeped_patches=26, dim_ratio=0.2):
        # MLPç”Ÿæˆèšåˆæƒé‡çŸ©é˜µ
        self.weight = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, int(dim * dim_ratio)),
            nn.GELU(),
            nn.Linear(int(dim * dim_ratio), keeped_patches),
        )
        self.scale = nn.Parameter(torch.ones(1, 1, 1))
```

### ä¿®å¤ 2ï¼šæ·»åŠ  `with torch.no_grad()`

**é—®é¢˜**ï¼šattentionè®¡ç®—æ²¡æœ‰ä½¿ç”¨no_gradï¼Œæµªè´¹æ˜¾å­˜å’Œè®¡ç®—
**ä¿®å¤**ï¼šæ‰€æœ‰attentionè®¡ç®—éƒ½æ·»åŠ äº†`with torch.no_grad()`

```python
def _compute_self_attention(self, patches, global_feat):
    with torch.no_grad():  # â† æ·»åŠ 
        patches_norm = F.normalize(patches, dim=-1)
        global_norm = F.normalize(global_feat, dim=-1)
        self_attn = (patches_norm * global_norm).sum(dim=-1)
    return self_attn
```

### ä¿®å¤ 3ï¼šä¿æŒåŸè®ºæ–‡çš„æ¯”ä¾‹

**é—®é¢˜**ï¼šå‚æ•°è®¾ç½®ä¸åŸè®ºæ–‡ä¸ä¸€è‡´
**ä¿®å¤**ï¼šä½¿ç”¨å’ŒåŸè®ºæ–‡ç›¸åŒçš„æ¯”ä¾‹

| å‚æ•° | åŸè®ºæ–‡(ViT) | æˆ‘ä»¬(RGBNT) | è¯´æ˜ |
|------|------------|-----------|------|
| **åˆå§‹patches** | 196 (14Ã—14) | 128 (16Ã—8) | - |
| **sparse_ratio** | 0.5 | 0.5 | âœ… ä¸€è‡´ |
| **aggr_ratio** | 0.4 | 0.4 | âœ… ä¸€è‡´ |
| **æœ€ç»ˆæ¯”ä¾‹** | 0.199 | 0.195 | âœ… å‡ ä¹ä¸€è‡´ |
| **N_s (é€‰ä¸­)** | 98 | 64 | - |
| **N_c (èšåˆ)** | 39 | 25 | - |
| **æœ€ç»ˆ+extra** | 40 | 26 | - |

### ä¿®å¤ 4ï¼šGumbel-Softmax çš„æ­£ç¡®ç†è§£

**Gumbelçš„ä½œç”¨**ï¼š
- âŒ **ä¸æ˜¯**ï¼šç”¨æ¥è½¯é€‰æ‹©tokensï¼ˆæˆ‘ä¹‹å‰çš„ç†è§£ï¼‰
- âœ… **è€Œæ˜¯**ï¼šç”Ÿæˆå¯å¾®çš„å†³ç­–çŸ©é˜µDï¼Œç”¨äºåç»­aggregationçš„mask
- âœ… **æœºåˆ¶**ï¼šStraight-Through Estimatorï¼ˆå‰å‘ç¡¬ï¼Œåå‘è½¯ï¼‰

```python
if self.use_gumbel:
    soft_mask = F.softmax((score + gumbel_noise) / tau, dim=1)
    hard_mask = torch.zeros_like(score).scatter(1, keep_policy, 1.0)
    score_mask = hard_mask + (soft_mask - soft_mask.detach())  # STE
```

---

## ğŸ“Š å®Œæ•´æµç¨‹ï¼ˆä»¥ RGB ä¸ºä¾‹ï¼‰

```
RGB_cash (B, 128, 512) + RGB_global (B, 512)
  â†“
[Attention Computation] with torch.no_grad():
  - rgb_self_attn: RGBè‡ªæ³¨æ„åŠ› s^{im}
  - rgb_nir_cross: NIRâ†’RGBäº¤å‰æ³¨æ„åŠ› s^{m2}
  - rgb_tir_cross: TIRâ†’RGBäº¤å‰æ³¨æ„åŠ› s^{m3}
  â†“
[Semantic Scoring] å…¬å¼1-3:
  - s^p = MLP(RGB_cash)
  - score = (1-2Î²)Â·s^p + Î²Â·(s^{m2} + s^{m3} + 2Â·s^{im})
  â†“
[TokenSparse] é€‰æ‹©æ˜¾è‘—patches:
  - Top-K: é€‰æ‹©scoreæœ€é«˜çš„64ä¸ªpatches
  - Gumbel: ç”Ÿæˆå¯å¾®å†³ç­–çŸ©é˜µD (B, 128)
  - è¾“å‡º: select_tokens (B, 64, 512)
         extra_token (B, 1, 512)
  â†“
[TokenAggregation] å…¬å¼4:
  - MLPç”Ÿæˆæƒé‡çŸ©é˜µ W: (B, 25, 64)
  - Softmaxå½’ä¸€åŒ–: Î£_i W_{ji} = 1
  - BMM: aggr_tokens = W @ select_tokens
  - è¾“å‡º: aggr_tokens (B, 25, 512)
  â†“
[Concatenation]
  RGB_enhanced = cat[aggr_tokens, extra_token]
  è¾“å‡º: (B, 26, 512)
```

åŒæ ·çš„æµç¨‹åº”ç”¨åˆ° NIR å’Œ TIRã€‚

---

## ğŸ“ å®Œæ•´çš„æ•°é‡å¯¹æ¯”

### åŸè®ºæ–‡ï¼ˆFlickr30K, ViT-Base-224ï¼‰

```
è¾“å…¥: 196 patches (14Ã—14 grid)
  â†“ TokenSparse (0.5)
98 patches (50%)
  â†“ TokenAggregation (0.4)
39 patches (20%)
  â†“ +extra
40 patches
  â†“ +[CLS]
41 patches â†’ ç”¨äºè®¡ç®—ç›¸ä¼¼åº¦

æœ€ç»ˆå‹ç¼©: 196 â†’ 41 = 0.209 (21%)
```

### æˆ‘ä»¬çš„å®ç°ï¼ˆRGBNT201, ViT-B-16ï¼‰

```
è¾“å…¥: 128 patches (16Ã—8 grid)
  â†“ TokenSparse (0.5)
64 patches (50%)
  â†“ TokenAggregation (0.4)
25 patches (19.5%)
  â†“ +extra
26 patches â†’ ç”¨äºpooling

æœ€ç»ˆå‹ç¼©: 128 â†’ 26 = 0.203 (20.3%)
```

âœ… **æ¯”ä¾‹å®Œå…¨ä¸€è‡´ï¼** (20.3% vs 20.9%)

---

## ğŸ¯ æ‚¨æå‡ºçš„ä¸¤ä¸ªé—®é¢˜çš„å®Œæ•´ç­”æ¡ˆ

### é—®é¢˜ 1ï¼šattention è®¡ç®—ä¸åŸç‰ˆçš„å¯¹æ¯”

#### âœ… æ ¸å¿ƒé€»è¾‘

```python
# åŸç‰ˆï¼ˆå¼€æº + è®ºæ–‡ç‰ˆæœ¬ï¼‰
with torch.no_grad():
    global_norm = F.normalize(global_feat.mean(...), dim=-1)
    attention = (global_norm * patches_norm).sum(dim=-1)

# æˆ‘çš„ä¿®å¤ç‰ˆ
def _compute_self_attention(self, patches, global_feat):
    with torch.no_grad():  # âœ… å·²æ·»åŠ 
        patches_norm = F.normalize(patches, dim=-1)
        global_norm = F.normalize(global_feat, dim=-1)
        return (patches_norm * global_norm).sum(dim=-1)
```

**ç­”æ¡ˆ**ï¼š
- âœ… è®¡ç®—æ–¹å¼å®Œå…¨ä¸€è‡´ï¼ˆL2å½’ä¸€åŒ– + ç‚¹ç§¯ï¼‰
- âœ… æ²¡æœ‰å¯å­¦ä¹ å‚æ•°ï¼ˆç¬¦åˆåŸç‰ˆï¼‰
- âœ… å·²æ·»åŠ  `with torch.no_grad()`ï¼ˆä¿®å¤å®Œæˆï¼‰

### é—®é¢˜ 2ï¼šGumbel-Softmax çš„ä½œç”¨

#### âœ… çœŸå®ä½œç”¨

Gumbel-Softmax **ä¸æ˜¯**ç”¨æ¥è½¯é€‰æ‹©tokensï¼Œè€Œæ˜¯ï¼š

1. **ç”Ÿæˆå¯å¾®çš„å†³ç­–çŸ©é˜µ D**
```python
hard_mask = Top-Ké€‰æ‹©çš„01çŸ©é˜µï¼ˆå‰å‘ï¼‰
soft_mask = Gumbel-Softmaxç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒï¼ˆåå‘ï¼‰
score_mask = hard_mask + (soft_mask - soft_mask.detach())  # STE
```

2. **ä¼ é€’ç»™ TokenAggregation**ï¼ˆå¯é€‰ï¼‰
```python
aggr_tokens = aggregation(select_tokens, keep_policy=score_mask)
```

3. **å…è®¸æ¢¯åº¦åå‘ä¼ æ’­**
   - å‰å‘ï¼šä½¿ç”¨ç¡¬å†³ç­–ï¼ˆTop-Kï¼Œç¡®å®šæ€§ï¼‰
   - åå‘ï¼šä½¿ç”¨è½¯æ¢¯åº¦ï¼ˆGumbelï¼Œå¯å¾®ï¼‰

#### å½“å‰çŠ¶æ€

- âœ… Gumbelç”Ÿæˆçš„score_maskæ˜¯å¯å¾®çš„
- âš ï¸ ä½†åœ¨å¼€æºä»£ç ä¸­ï¼Œaggregation**ä¸ä½¿ç”¨**keep_policy
- âš ï¸ æ‰€ä»¥Gumbelåœ¨å¼€æºç‰ˆæœ¬ä¸­ä¹Ÿæ˜¯æ— æ•ˆçš„
- âœ… æˆ‘ä»¬çš„å®ç°ä¿ç•™äº†è¿™ä¸ªé€‰é¡¹ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦ä½¿ç”¨

---

## ğŸ“Š æœ€ç»ˆå®ç°å¯¹æ¯”åŸè®ºæ–‡

| ç‰¹æ€§ | åŸè®ºæ–‡è¦æ±‚ | å¼€æºä»£ç  | æˆ‘çš„å®Œæ•´ç‰ˆ | çŠ¶æ€ |
|------|----------|---------|-----------|------|
| **MLP Predictor** | âœ… | âŒ | âœ… | âœ… |
| **å¤šæºattention** | âœ… | âœ… | âœ… (æ”¹ä¸ºå¤šæ¨¡æ€) | âœ… |
| **ç»¼åˆå¾—åˆ†å…¬å¼** | âœ… | âŒ | âœ… | âœ… |
| **with no_grad** | - | âš ï¸ | âœ… | âœ… |
| **Gumbel-Softmax** | âœ… | âŒ | âœ… | âœ… |
| **TokenAggregation** | âœ… | âœ… | âœ… | âœ… |
| **æœ€ç»ˆæ¯”ä¾‹** | ~20% | ~20% | ~20% | âœ… |

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒ

```bash
python train_net.py --config_file configs/RGBNT201/DeMo_SDTPS.yml
```

### é…ç½®å‚æ•°

```yaml
MODEL:
  USE_SDTPS: True
  SDTPS_SPARSE_RATIO: 0.5  # é€‰æ‹©50%
  SDTPS_AGGR_RATIO: 0.4    # èšåˆåˆ°40%
  SDTPS_BETA: 0.25         # æƒé‡å‚æ•°
  SDTPS_USE_GUMBEL: False  # Gumbel-Softmaxå¼€å…³
```

### é¢„æœŸæ•ˆæœ

- âœ… æ¯ä¸ªæ¨¡æ€å‹ç¼©ï¼š128 â†’ 26 patchesï¼ˆå‹ç¼©80%ï¼‰
- âœ… è®¡ç®—é‡å¤§å¹…å‡å°‘
- âœ… è·¨æ¨¡æ€å¼•å¯¼çš„æ™ºèƒ½é€‰æ‹©
- âœ… å­¦ä¹ çš„èšåˆç­–ç•¥

---

## ğŸ“ å›ç­”æ‚¨çš„å…·ä½“é—®é¢˜

### å…³äº "Då’Œweight matrixåšelementwise"

æ ¹æ®æˆ‘å¯¹åŸä»£ç çš„ä»”ç»†é˜…è¯»ï¼Œå®é™…æµç¨‹æ˜¯ï¼š

1. **Dï¼ˆå†³ç­–çŸ©é˜µï¼‰**ï¼šTokenSparseè¾“å‡ºçš„score_mask
2. **Wï¼ˆæƒé‡çŸ©é˜µï¼‰**ï¼šTokenAggregationç”Ÿæˆçš„èšåˆæƒé‡
3. **ç»“åˆæ–¹å¼**ï¼š**ä¸æ˜¯elementwise**ï¼Œè€Œæ˜¯ï¼š
   ```python
   weight = weight - (1 - D) * 1e10  # ç”¨D mask W
   weight = F.softmax(weight, dim=2)  # å½’ä¸€åŒ–
   output = torch.bmm(weight, tokens)  # çŸ©é˜µä¹˜æ³•
   ```

**ä½†æ˜¯**ï¼Œåœ¨å¼€æºä»£ç ä¸­ï¼Œaggregationè°ƒç”¨æ—¶**æ²¡æœ‰ä¼ é€’keep_policy**ï¼š
```python
aggr_tokens = self.aggr_net(select_tokens)  # æ²¡æœ‰ä¼ D
```

æ‰€ä»¥Dï¼ˆscore_maskï¼‰åœ¨å¼€æºç‰ˆæœ¬ä¸­ä¸»è¦ç”¨äºï¼š
- è®°å½•å“ªäº›patchesè¢«é€‰ä¸­ï¼ˆç”¨äºlossè®¡ç®—ï¼‰
- ä¸ç›´æ¥å½±å“aggregation

å¦‚æœæ‚¨çš„ç†è§£ä¸åŒï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“çš„æµç¨‹ï¼Œæˆ‘ä¼šæ®æ­¤è°ƒæ•´ï¼

---

## âœ… å½“å‰å®ç°æ€»ç»“

| ç»„ä»¶ | è¾“å…¥ | è¾“å‡º | çŠ¶æ€ |
|------|------|------|------|
| **TokenSparse** | (B,128,512) | (B,64,512) | âœ… |
| **TokenAggregation** | (B,64,512) | (B,25,512) | âœ… |
| **+extra_token** | - | (B,26,512) | âœ… |
| **Mean pool** | (B,26,512) | (B,512) | âœ… |
| **Concat 3 modalities** | - | (B,1536) | âœ… |

**æœ€ç»ˆæ¯”ä¾‹**: 128 â†’ 26 = **20.3%** âœ…ï¼ˆå’ŒåŸè®ºæ–‡çš„20%ä¸€è‡´ï¼‰

æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼ğŸš€
