# SDTPS ä¸‰æ–¹å¯¹æ¯”åˆ†æï¼šè®ºæ–‡ vs è®ºæ–‡ç‰ˆæœ¬ä»£ç  vs å¼€æºä»£ç 

## å®Œæ•´æµç¨‹å¯¹æ¯”

### ğŸ“– è®ºæ–‡æè¿°ï¼ˆiclr2026_conference.texï¼‰

#### Stage 1: Semantic Scoring (å…¬å¼ 1-3)

**å…¬å¼ 1 - MLP é¢„æµ‹**:
```
s_i^p = Ïƒ(MLP(v_i))
```

**å…¬å¼ 2 - å¤šæºæ³¨æ„åŠ›**:
```
s_i^{st} = Norm(v_i^T Â· E_{st} / d)  # ç¨€ç–æ–‡æœ¬
s_i^{dt} = Norm(v_i^T Â· E_{dt} / d)  # ç¨ å¯†æ–‡æœ¬
s_i^{im} = Norm(v_i^T Â· E_{im} / d)  # å›¾åƒè‡ªæ³¨æ„åŠ›
```

**å…¬å¼ 3 - ç»¼åˆå¾—åˆ†**:
```
s_i = (1-2Î²)Â·s_i^p + Î²Â·(s_i^{st} + s_i^{dt} + 2Â·s_i^{im})
```

#### Stage 2: Decision and Aggregation

**å†³ç­–**ï¼š
- "Gumbel-Softmax technique provides smooth and differentiable sampling capabilities"
- ç”Ÿæˆå†³ç­–çŸ©é˜µ D_s å’Œ D_dï¼ˆone-hotï¼Œ1=é€‰ä¸­ï¼Œ0=ä¸¢å¼ƒï¼‰
- åŸºäº D é€‰æ‹©æ˜¾è‘— patches: V_s, V_d

**å…¬å¼ 4 - èšåˆ**:
```
vÌ‚_j = Î£_{i=1}^{N_s} (W_s)_{ij}Â·v_i^s + Î£_{i=1}^{N_d} (W_d)_{ij}Â·v_i^d
```

å…¶ä¸­ï¼š
- W_s âˆˆ R^{N_s Ã— N_c}, W_d âˆˆ R^{N_d Ã— N_c}
- Î£_i (W_s)_{ij} = 1, Î£_i (W_d)_{ij} = 1
- W_s = Softmax(MLP(V_s))
- W_d = Softmax(MLP(V_d))
- N_c < max(N_s, N_d)

---

## ğŸ” ä¸‰ä¸ªå®ç°ç‰ˆæœ¬çš„å¯¹æ¯”

### 1ï¸âƒ£ å¼€æºä»£ç ç‰ˆæœ¬ï¼ˆcross_net.pyï¼‰

```python
class CrossSparseAggrNet_v2(nn.Module):
    def __init__(self, opt):
        # å…³é”®å‚æ•°
        self.sparse_ratio = opt.sparse_ratio    # 0.5
        self.aggr_ratio = opt.aggr_ratio        # 0.4
        self.keeped_patches = int(self.num_patches * self.aggr_ratio * self.sparse_ratio)
        # = int(196 Ã— 0.4 Ã— 0.5) = 39

        # Stage 1: Sparse
        self.sparse_net_cap = TokenSparse(sparse_ratio=0.5)
        self.sparse_net_long = TokenSparse(sparse_ratio=0.5)

        # Stage 2: Aggregationï¼ˆå•æƒé‡ç‰ˆæœ¬ï¼‰
        self.aggr_net = TokenAggregation(keeped_patches=39)

    def forward(self, img_embs, cap_embs, cap_lens, long_cap_embs, long_cap_lens):
        # è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼ˆæ—  no_gradï¼‰
        img_spatial_self_attention = (img_spatial_glo_norm * img_spatial_embs_norm).sum(dim=-1)

        for i in range(len(cap_lens)):
            # 1. è®¡ç®—äº¤å‰æ³¨æ„åŠ›ï¼ˆæœ‰ no_gradï¼‰
            with torch.no_grad():
                cap_i_glo = F.normalize(cap_i.mean(0, keepdim=True).unsqueeze(0), dim=-1)
                attn_cap = (cap_i_glo * img_spatial_embs_norm).sum(dim=-1)

                # 2. TokenSparse
                select_tokens_cap, extra_token_cap, score_mask_cap = self.sparse_net_cap(
                    tokens=img_spatial_embs,
                    attention_x=img_spatial_self_attention,
                    attention_y=attn_cap,
                )
                # select_tokens_cap: (B, 98, C)

            # 3. TokenAggregation â† å…³é”®ï¼
            aggr_tokens = self.aggr_net(select_tokens_cap)
            # aggr_tokens: (B, 39, C)

            # 4. æ·»åŠ  extra_token
            keep_spatial_tokens = torch.cat([aggr_tokens, extra_token_cap], dim=1)
            # keep_spatial_tokens: (B, 40, C)

        # ç¨ å¯†æ–‡æœ¬åˆ†æ”¯åŒç†
        for i in range(len(long_cap_lens)):
            ...
```

**ç‰¹ç‚¹**ï¼š
- âœ… æœ‰ aggregation
- âŒ è‡ªæ³¨æ„åŠ›è®¡ç®—**æ²¡æœ‰** `no_grad`
- âŒ äº¤å‰æ³¨æ„åŠ›è®¡ç®—**åœ¨ no_grad å†…**
- âŒ ä½¿ç”¨**å•ä¸ª** aggregation ç½‘ç»œï¼ˆè®ºæ–‡è¦æ±‚åŒåˆ†æ”¯ï¼‰
- âŒ æ²¡æœ‰ä½¿ç”¨è®ºæ–‡å…¬å¼(1)çš„ MLP predictor
- âŒ æ²¡æœ‰ Gumbel-Softmax

---

### 2ï¸âƒ£ è®ºæ–‡ç‰ˆæœ¬ï¼ˆseps_modules_reviewed_v2_enhanced.pyï¼‰

```python
class CrossSparseAggrNet(nn.Module):
    def __init__(self, use_paper_version=True, use_dual_aggr=True, use_gumbel_softmax=True):
        self.keeped_patches = int(num_patches * aggr_ratio * sparse_ratio)

        # Stage 1: TokenSparseï¼ˆæ”¯æŒè®ºæ–‡ç‰ˆæœ¬ï¼‰
        self.sparse_net_cap = TokenSparse(
            use_paper_version=use_paper_version  # â† æ”¯æŒ MLP predictor
        )

        # Stage 2: Aggregationï¼ˆæ”¯æŒåŒåˆ†æ”¯ï¼‰
        if use_paper_version and use_dual_aggr:
            # è®ºæ–‡ç‰ˆæœ¬ï¼šåŒåˆ†æ”¯èšåˆ
            self.aggr_net = DualTokenAggregation(keeped_patches=...)
        else:
            # å¼€æºç‰ˆæœ¬ï¼šå•åˆ†æ”¯èšåˆ
            self.aggr_net = TokenAggregation(keeped_patches=...)

    def forward(self, ...):
        # è®¡ç®—è‡ªæ³¨æ„åŠ›
        with torch.no_grad():  # âœ… æœ‰ no_grad
            img_spatial_self_attention = ...

        for i in range(len(cap_lens)):
            # è®¡ç®—äº¤å‰æ³¨æ„åŠ›
            with torch.no_grad():  # âœ… æœ‰ no_grad
                attn_cap = ...
                dense_attn = ...

            # TokenSparse
            select_tokens_cap, extra_token_cap, score_mask_cap = self.sparse_net_cap(
                tokens=img_spatial_embs,
                attention_x=img_spatial_self_attention,
                attention_y=attn_cap,
                attention_y_dense=dense_attn,  # â† æ”¯æŒç¨ å¯†æ–‡æœ¬
                beta=self.beta,
                use_gumbel=self.use_gumbel_softmax,  # â† æ”¯æŒ Gumbel
            )

            # TokenAggregation
            if use_paper_version and use_dual_aggr:
                # åŒåˆ†æ”¯èšåˆ
                aggr_tokens = self.aggr_net(
                    select_tokens_cap,   # V_s
                    select_tokens_long,  # V_d
                )
            else:
                # å•åˆ†æ”¯èšåˆ
                aggr_tokens = self.aggr_net(select_tokens_cap)

            keep_spatial_tokens = torch.cat([aggr_tokens, extra_token], dim=1)
```

**ç‰¹ç‚¹**ï¼š
- âœ… å®Œæ•´å®ç°è®ºæ–‡æ‰€æœ‰ç‰¹æ€§
- âœ… æ”¯æŒ MLP predictorï¼ˆå…¬å¼1ï¼‰
- âœ… æ”¯æŒåŒåˆ†æ”¯èšåˆï¼ˆå…¬å¼4ï¼‰
- âœ… æ‰€æœ‰ attention è®¡ç®—éƒ½åœ¨ `no_grad` å†…
- âœ… æ”¯æŒ Gumbel-Softmax

---

### 3ï¸âƒ£ æˆ‘çš„å®ç°ï¼ˆmodeling/sdtps.pyï¼‰

```python
class MultiModalSDTPS(nn.Module):
    def __init__(self, ...):
        # åªæœ‰ TokenSparse
        self.rgb_sparse = TokenSparse(...)
        self.nir_sparse = TokenSparse(...)
        self.tir_sparse = TokenSparse(...)

        # âŒ æ²¡æœ‰ aggregation ç½‘ç»œ

    def forward(self, RGB_cash, NI_cash, TI_cash, RGB_global, NI_global, TI_global):
        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆâŒ æ²¡æœ‰ no_gradï¼‰
        rgb_self_attn = self._compute_self_attention(RGB_cash, RGB_global)
        rgb_nir_cross = self._compute_cross_attention(RGB_cash, NI_global)

        # TokenSparse
        rgb_select, rgb_extra, rgb_mask = self.rgb_sparse(...)
        # rgb_select: (B, 77, C)

        # âŒ ç›´æ¥æ‹¼æ¥ï¼Œæ²¡æœ‰ aggregation
        RGB_enhanced = torch.cat([rgb_select, rgb_extra], dim=1)
        # RGB_enhanced: (B, 78, C)
```

**ç‰¹ç‚¹**ï¼š
- âœ… é€‚é…äº†å¤šæ¨¡æ€è¾“å…¥
- âŒ æ²¡æœ‰ aggregation
- âŒ æ²¡æœ‰ `no_grad`
- âŒ æ²¡æœ‰ Gumbel-Softmax çš„çœŸæ­£å¯å¾®æ€§

---

## ğŸ“Š è¯¦ç»†å¯¹æ¯”è¡¨

| ç‰¹æ€§ | è®ºæ–‡æè¿° | å¼€æºä»£ç  | è®ºæ–‡ç‰ˆæœ¬ä»£ç  | æˆ‘çš„å®ç° |
|------|---------|---------|------------|---------|
| **MLP Predictor (å…¬å¼1)** | âœ… æœ‰ | âŒ æ—  | âœ… å¯é€‰ | âœ… æœ‰ |
| **ç¨ å¯†æ–‡æœ¬ (s^dt)** | âœ… æœ‰ | âŒ æ—  | âœ… å¯é€‰ | âŒ æ—  |
| **Gumbel-Softmax** | âœ… æœ‰ | âŒ æ—  | âœ… å¯é€‰ | âš ï¸ æœ‰ä½†æ— æ•ˆ |
| **Self-Attention no_grad** | - | âŒ æ—  | âœ… æœ‰ | âŒ æ—  |
| **Cross-Attention no_grad** | - | âœ… æœ‰ | âœ… æœ‰ | âŒ æ—  |
| **TokenAggregation** | âœ… æœ‰(å…¬å¼4) | âœ… å•åˆ†æ”¯ | âœ… åŒåˆ†æ”¯ | âŒ **å®Œå…¨ç¼ºå¤±** |
| **Dual Aggregation (W_s+W_d)** | âœ… æœ‰ | âŒ æ—  | âœ… å¯é€‰ | âŒ æ—  |

---

## ğŸ¯ æˆ‘éœ€è¦ä¿®å¤çš„å†…å®¹

### å¿…é¡»ä¿®å¤ï¼ˆè®ºæ–‡è¦æ±‚ï¼‰

1. âœ… **æ·»åŠ  TokenAggregation**
   - ä» 98 patches â†’ 39 patchesï¼ˆè¿›ä¸€æ­¥å‹ç¼©ï¼‰
   - å­¦ä¹ èšåˆæƒé‡çŸ©é˜µ

2. âœ… **æ·»åŠ  `with torch.no_grad()`**
   - æ‰€æœ‰ attention è®¡ç®—éƒ½åº”è¯¥åœ¨ no_grad å†…

3. âš ï¸ **ä¿®å¤ Gumbel-Softmax**ï¼ˆå¦‚æœè¦ç”¨ï¼‰
   - å½“å‰è™½ç„¶è®¡ç®—äº†ä½†æ²¡æœ‰çœŸæ­£å‘æŒ¥ä½œç”¨

### å¯é€‰ï¼ˆæ ¹æ®éœ€æ±‚ï¼‰

4. âŒ **Dual Aggregation**ï¼ˆåŒåˆ†æ”¯ W_s + W_dï¼‰
   - æˆ‘ä»¬åªæœ‰ä¸‰ä¸ªæ¨¡æ€ï¼Œä¸éœ€è¦ç¨€ç–/ç¨ å¯†æ–‡æœ¬çš„åŒºåˆ†
   - å¯ä»¥ç”¨å•åˆ†æ”¯ aggregation

---

## ğŸ“ ä¿®å¤åçš„å®Œæ•´æ•°é‡å˜åŒ–

### å¤šæ¨¡æ€ ReID åœºæ™¯ï¼ˆä»¥ RGB ä¸ºä¾‹ï¼‰

```
è¾“å…¥: RGB_cash (B, 128, 512)
  â†“ [Stage 1: TokenSparse]
  sparse_ratio = 0.6
  N_s = ceil(128 Ã— 0.6) = 77
  â†“
select_tokens (B, 77, 512)
  â†“ [Stage 2: TokenAggregation] â† ç¼ºå°‘è¿™ä¸€æ­¥ï¼
  aggr_ratio = 0.4
  N_c = int(128 Ã— 0.4 Ã— 0.6) = 30
  â†“
aggr_tokens (B, 30, 512)
  â†“ [Stage 3: æ·»åŠ  extra_token]
  â†“
enhanced_tokens (B, 31, 512)
```

**å¯¹æ¯”**ï¼š
- å½“å‰å®ç°ï¼š(B, 78, 512) - **è¿‡å¤§**
- ä¿®å¤åï¼š(B, 31, 512) - **ç¬¦åˆè®ºæ–‡**

---

## ğŸ” TokenAggregation çš„è¯¦ç»†å®ç°

### è®ºæ–‡å…¬å¼ï¼ˆå…¬å¼4ï¼‰

```
vÌ‚_j = Î£_{i=1}^{N_s} (W_s)_{ij} Â· v_i^s + Î£_{i=1}^{N_d} (W_d)_{ij} Â· v_i^d
```

### å¼€æºä»£ç å®ç°ï¼ˆcross_net.py: Line 61-97ï¼‰

```python
class TokenAggregation(nn.Module):
    def __init__(self, dim=512, keeped_patches=64, dim_ratio=0.2):
        hidden_dim = int(dim * dim_ratio)  # 512 Ã— 0.2 = 102

        # MLP ç”Ÿæˆèšåˆæƒé‡
        self.weight = nn.Sequential(
            nn.LayerNorm(dim),              # å½’ä¸€åŒ–
            nn.Linear(dim, hidden_dim),     # 512 â†’ 102
            nn.GELU(),                       # æ¿€æ´»
            nn.Linear(hidden_dim, keeped_patches)  # 102 â†’ N_c
        )

        self.scale = nn.Parameter(torch.ones(1, 1, 1))  # å¯å­¦ä¹ ç¼©æ”¾

    def forward(self, x, keep_policy=None):
        # x: (B, N_s, C)

        # ç”Ÿæˆæƒé‡çŸ©é˜µ
        weight = self.weight(x)           # (B, N_s, C) â†’ (B, N_s, N_c)
        weight = weight.transpose(2, 1)   # (B, N_s, N_c) â†’ (B, N_c, N_s)
        weight = weight * self.scale      # ç¼©æ”¾

        # å¦‚æœæœ‰ maskï¼Œå±è”½æ— æ•ˆä½ç½®
        if keep_policy is not None:
            keep_policy = keep_policy.unsqueeze(1)  # (B, N_s) â†’ (B, 1, N_s)
            weight = weight - (1 - keep_policy) * 1e10

        # Softmax å½’ä¸€åŒ–ï¼ˆä¿è¯ Î£_i W_{ji} = 1ï¼‰
        weight = F.softmax(weight, dim=2)  # (B, N_c, N_s)

        # æ‰¹é‡çŸ©é˜µä¹˜æ³•
        return torch.bmm(weight, x)  # (B, N_c, N_s) @ (B, N_s, C) â†’ (B, N_c, C)
```

**æ•°å­¦è§£é‡Š**ï¼š
```
è¾“å…¥: x = [v_1, v_2, ..., v_{N_s}] (B, N_s, C)

MLP: x â†’ logits (B, N_s, N_c)
     æ¯ä¸ª v_i ç”Ÿæˆ N_c ä¸ªæƒé‡å€¼

Transpose: (B, N_s, N_c) â†’ (B, N_c, N_s)
           é‡æ’ä¸ºæ¯ä¸ªè¾“å‡ºä½ç½® j å¯¹åº” N_s ä¸ªè¾“å…¥æƒé‡

Softmax: W[b,j,:] = softmax(logits[b,:,j])
         ä¿è¯ Î£_i W[b,j,i] = 1

BMM: vÌ‚_j = Î£_i W[b,j,i] Ã— v_i
```

### è®ºæ–‡ç‰ˆæœ¬å®ç°ï¼ˆv2_enhanced: DualTokenAggregationï¼‰

```python
class DualTokenAggregation(nn.Module):
    """åŒåˆ†æ”¯èšåˆ - å®Œæ•´è®ºæ–‡ç‰ˆæœ¬"""
    def __init__(self, dim=512, keeped_patches=64):
        # ç¨€ç–æ–‡æœ¬åˆ†æ”¯
        self.weight_sparse = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, keeped_patches),
        )

        # ç¨ å¯†æ–‡æœ¬åˆ†æ”¯
        self.weight_dense = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, keeped_patches),
        )

    def forward(self, tokens_sparse, tokens_dense, mask_sparse, mask_dense):
        # åˆ†åˆ«èšåˆä¸¤ä¸ªåˆ†æ”¯
        out_s = self._aggregate(tokens_sparse, mask_sparse, self.weight_sparse)
        out_d = self._aggregate(tokens_dense, mask_dense, self.weight_dense)

        # ä¸¤ä¸ªåˆ†æ”¯ç›¸åŠ 
        return out_s + out_d  # (B, N_c, C)
```

---

## ğŸ” Gumbel-Softmax çš„çœŸå®ç”¨æ³•

### è®ºæ–‡ç‰ˆæœ¬çš„å®ç°ï¼ˆv2_enhanced: Line 284-315ï¼‰

```python
# TokenSparse.forward() ä¸­
if use_gumbel:
    # 1. Gumbel-Softmaxï¼ˆç”Ÿæˆè½¯ maskï¼‰
    gumbel_noise = -torch.log(-torch.log(torch.rand_like(score) + 1e-9) + 1e-9)
    soft_mask = F.softmax((score + gumbel_noise) / gumbel_tau, dim=1)

    # 2. Hard maskï¼ˆTop-Kï¼‰
    hard_mask = torch.zeros_like(score).scatter(1, keep_policy, 1.0)

    # 3. Straight-Through Estimator
    score_mask = hard_mask + (soft_mask - soft_mask.detach())
    # å‰å‘ï¼šä½¿ç”¨ hard_maskï¼ˆç¡®å®šæ€§ï¼‰
    # åå‘ï¼šä½¿ç”¨ soft_maskï¼ˆå¯å¾®ï¼‰

# â— å…³é”®ï¼šscore_mask ç”¨åœ¨å“ªé‡Œï¼Ÿ
# åœ¨ TokenAggregation.forward() ä¸­ï¼š
weight = weight - (1 - keep_policy) * 1e10  # â† è¿™é‡Œç”¨ score_mask ä½œä¸º keep_policy
```

**Gumbel çš„çœŸå®ä½œç”¨**ï¼š
- ä¸æ˜¯ç”¨æ¥é€‰æ‹© tokenï¼ˆä»ç„¶ç”¨ Top-Kï¼‰
- è€Œæ˜¯ç”Ÿæˆä¸€ä¸ª**å¯å¾®çš„ mask**
- è¿™ä¸ª mask ä¼ é€’ç»™ **TokenAggregation**
- åœ¨ aggregation çš„ softmax ä¹‹å‰å±è”½æ— æ•ˆä½ç½®
- é€šè¿‡ STE è®©æ¢¯åº¦èƒ½å¤Ÿåå‘ä¼ æ’­åˆ° score è®¡ç®—

---

## âœ… æ­£ç¡®çš„å®Œæ•´æµç¨‹

### è®ºæ–‡è¦æ±‚çš„å®Œæ•´æµç¨‹

```python
# Stage 1: TokenSparse
select_tokens, extra_token, score_mask = TokenSparse(
    tokens=patches,
    attention_x=self_attn,
    attention_y=cross_attn_m2,
    attention_y_dense=cross_attn_m3,
    use_gumbel=True,
)
# select_tokens: (B, N_s, C)
# score_mask: (B, N) - å¯å¾®çš„å†³ç­–çŸ©é˜µ

# Stage 2: TokenAggregation
aggr_tokens = TokenAggregation(
    x=select_tokens,
    keep_policy=score_mask,  # â† ä½¿ç”¨ Gumbel ç”Ÿæˆçš„ mask
)
# aggr_tokens: (B, N_c, C)

# Stage 3: æ‹¼æ¥
final_tokens = torch.cat([aggr_tokens, extra_token], dim=1)
# final_tokens: (B, N_c+1, C)
```

---

## ğŸš¨ æ€»ç»“ï¼šæˆ‘çš„å®ç°ç¼ºå¤±çš„å…³é”®éƒ¨åˆ†

### 1. âŒ å®Œå…¨ç¼ºå¤± TokenAggregation
- **å½±å“**ï¼šè¾“å‡º patch æ•°é‡è¿‡å¤šï¼ˆ78 vs 31ï¼‰
- **ä¿®å¤**ï¼šæ·»åŠ  TokenAggregation ç½‘ç»œ

### 2. âŒ ç¼ºå°‘ `with torch.no_grad()`
- **å½±å“**ï¼šæ˜¾å­˜å ç”¨å¢åŠ ï¼Œå¯èƒ½å¹²æ‰°è®­ç»ƒ
- **ä¿®å¤**ï¼šåœ¨æ‰€æœ‰ attention è®¡ç®—ä¸­æ·»åŠ 

### 3. âš ï¸ Gumbel-Softmax æœªæ­£ç¡®ä½¿ç”¨
- **å½±å“**ï¼šè™½ç„¶è®¡ç®—äº†ä½†æ²¡å‘æŒ¥ä½œç”¨
- **ä¿®å¤**ï¼šå°† score_mask ä¼ é€’ç»™ TokenAggregation

### 4. âŒ ç¼ºå°‘ aggr_ratio å‚æ•°
- **å½±å“**ï¼šæ— æ³•æ§åˆ¶æœ€ç»ˆçš„ patch æ•°é‡
- **ä¿®å¤**ï¼šæ·»åŠ é…ç½®å‚æ•°

---

## ä¸‹ä¸€æ­¥ï¼šéœ€è¦æˆ‘åˆ›å»ºå®Œæ•´çš„ä¿®å¤ç‰ˆæœ¬å—ï¼Ÿ

åŒ…æ‹¬ï¼š
1. âœ… æ·»åŠ  TokenAggregationï¼ˆå•åˆ†æ”¯ï¼Œé€‚é…å¤šæ¨¡æ€ï¼‰
2. âœ… æ·»åŠ  `with torch.no_grad()`
3. âœ… æ­£ç¡®ä½¿ç”¨ Gumbel-Softmax + score_mask
4. âœ… æ·»åŠ  aggr_ratio é…ç½®
5. âœ… å®Œæ•´æµ‹è¯•éªŒè¯
