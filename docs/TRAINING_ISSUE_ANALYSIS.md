# SDTPS è®­ç»ƒé—®é¢˜åˆ†æ

## é—®é¢˜ç°è±¡

```
Epoch 4:
  Loss: 4.529ï¼ˆéå¸¸é«˜ï¼‰
  Acc: 0.050ï¼ˆ5%ï¼Œå‡ ä¹éšæœºï¼‰

201ä¸ªç±»åˆ«ï¼ŒéšæœºçŒœæµ‹å‡†ç¡®ç‡ â‰ˆ 0.5%
5% è¯´æ˜æ¨¡å‹å‡ ä¹æ²¡æœ‰å­¦åˆ°ä»»ä½•ä¸œè¥¿
```

## ğŸ” å…³é”®é—®é¢˜ï¼šAttention çš„ no_grad é˜»æ–­äº†å­¦ä¹ è·¯å¾„

### é—®é¢˜åˆ†æ

æˆ‘ä»¬åœ¨æ‰€æœ‰ attention è®¡ç®—ä¸­ä½¿ç”¨äº† `with torch.no_grad()`ï¼š

```python
def _compute_self_attention(self, patches, global_feat):
    with torch.no_grad():  # â† å…³é”®é—®é¢˜ï¼
        patches_norm = F.normalize(patches, dim=-1)
        global_norm = F.normalize(global_feat, dim=-1)
        return (patches_norm * global_norm).sum(dim=-1)
```

**è¿™å¯¼è‡´çš„æ¢¯åº¦æµåŠ¨**ï¼š

```
Backbone â†’ patches, global_feat
              â†“ (no_grad)
          attention scores
              â†“ (no_grad)
          ç»¼åˆ score
              â†“ (æœ‰æ¢¯åº¦)
          MLP predictor
              â†“
          é€‰æ‹© + èšåˆ
              â†“
          loss
```

**é—®é¢˜**ï¼š
- âŒ Backbone æ— æ³•ä» attention è·å¾—æ¢¯åº¦åé¦ˆ
- âŒ Backbone ä¸çŸ¥é“åº”è¯¥æå–ä»€ä¹ˆæ ·çš„ç‰¹å¾æ‰èƒ½è®© attention å·¥ä½œå¾—æ›´å¥½
- âŒ åªæœ‰ MLP predictor å’Œ aggregation æœ‰æ¢¯åº¦ï¼Œä½†å®ƒä»¬ä¾èµ– Backbone çš„ç‰¹å¾è´¨é‡

### åŸè®ºæ–‡ vs æˆ‘ä»¬çš„åœºæ™¯

| ç‰¹æ€§ | åŸ SEPSï¼ˆå›¾åƒ-æ–‡æœ¬æ£€ç´¢ï¼‰ | æˆ‘ä»¬ DeMoï¼ˆé‡è¯†åˆ«ï¼‰ |
|------|----------------------|------------------|
| Backbone | é¢„è®­ç»ƒçš„ ViTï¼Œ**å†»ç»“** | é¢„è®­ç»ƒçš„ ViTï¼Œ**éœ€è¦ finetune** |
| ä»»åŠ¡ | æ£€ç´¢ï¼ˆå›ºå®šç‰¹å¾ç©ºé—´ï¼‰ | åˆ†ç±»ï¼ˆå­¦ä¹ åˆ¤åˆ«ç‰¹å¾ï¼‰ |
| Attention ç›®çš„ | æ‰¾åˆ°ä¸æ–‡æœ¬å¯¹åº”çš„ patch | æ‰¾åˆ°åˆ¤åˆ«æ€§ patch |
| æ˜¯å¦éœ€è¦ Backbone å­¦ä¹  | âŒ ä¸éœ€è¦ | âœ… **éœ€è¦** |

**å…³é”®å·®å¼‚**ï¼š
- åŸ SEPS çš„ Backbone å·²ç»è®­ç»ƒå¥½ï¼Œåªéœ€è¦"é€‰æ‹©"æ­£ç¡®çš„ patches
- æˆ‘ä»¬çš„ Backbone éœ€è¦å­¦ä¹ æå–åˆ¤åˆ«æ€§ç‰¹å¾ï¼Œ**éœ€è¦æ¢¯åº¦æŒ‡å¯¼**ï¼

## ğŸ¯ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šç§»é™¤ attention è®¡ç®—ä¸­çš„ `no_grad`ï¼ˆæ¨èï¼‰

```python
def _compute_self_attention(self, patches, global_feat):
    # ç§»é™¤ with torch.no_grad()
    patches_norm = F.normalize(patches, dim=-1)
    global_norm = F.normalize(global_feat, dim=-1)
    return (patches_norm * global_norm).sum(dim=-1)
```

**ä¼˜ç‚¹**ï¼š
- âœ… Backbone èƒ½è·å¾—æ¢¯åº¦ï¼Œå­¦ä¹ åˆ¤åˆ«æ€§ç‰¹å¾
- âœ… ç«¯åˆ°ç«¯è®­ç»ƒ
- âœ… æ›´é€‚åˆé‡è¯†åˆ«ä»»åŠ¡

**ç¼ºç‚¹**ï¼š
- âš ï¸ æ˜¾å­˜å ç”¨å¢åŠ ï¼ˆéœ€è¦ä¿å­˜ä¸­é—´æ¢¯åº¦ï¼‰
- âš ï¸ è®¡ç®—ç¨æ…¢

### æ–¹æ¡ˆ2ï¼šåªç§»é™¤ self-attention çš„ `no_grad`

```python
def _compute_self_attention(self, patches, global_feat):
    # å…è®¸æ¢¯åº¦
    patches_norm = F.normalize(patches, dim=-1)
    global_norm = F.normalize(global_feat, dim=-1)
    return (patches_norm * global_norm).sum(dim=-1)

def _compute_cross_attention(self, patches, cross_global):
    with torch.no_grad():  # ä¿ç•™ cross-attention çš„ no_grad
        ...
```

### æ–¹æ¡ˆ3ï¼šå¢åŠ  warm-up é˜¶æ®µ

å‰å‡ ä¸ª epoch å…ˆè®­ç»ƒ Backbone å’Œ ç›´æ¥æ‹¼æ¥åˆ†æ”¯ï¼Œå†å¯ç”¨ SDTPSï¼š

```python
if epoch < 5:
    # åªç”¨ ori_score, ori è®¡ç®—æŸå¤±
    loss = loss_fn(ori_score, ori, target)
else:
    # åŒæ—¶ç”¨ SDTPS å’Œ ori
    loss = loss_fn(sdtps_score, sdtps_feat, target) + loss_fn(ori_score, ori, target)
```

### æ–¹æ¡ˆ4ï¼šè°ƒæ•´ sparse/aggr æ¯”ä¾‹

å½“å‰å‹ç¼©å¤ªæ¿€è¿›ï¼š128 â†’ 25 (19.5%)

å¯ä»¥å°è¯•ï¼š
```yaml
SDTPS_SPARSE_RATIO: 0.7  # 70%
SDTPS_AGGR_RATIO: 0.5    # 50%
# æœ€ç»ˆï¼š128 â†’ 90 â†’ 45 (35%)
```

### æ–¹æ¡ˆ5ï¼šæ£€æŸ¥æ˜¯å¦çœŸçš„åœ¨ä½¿ç”¨ SDTPS ç‰¹å¾

æ£€æŸ¥ processor.py line 47:
```python
test_sign = cfg.MODEL.HDM or cfg.MODEL.ATM  # â† æ²¡æœ‰æ£€æŸ¥ USE_SDTPSï¼
```

è™½ç„¶è¿™åªå½±å“è¯„ä¼°ï¼Œä½†å¯èƒ½æœ‰å…¶ä»–åœ°æ–¹ä¹Ÿæœ‰ç±»ä¼¼é—®é¢˜ã€‚

## ğŸ”§ ç«‹å³å°è¯•çš„ä¿®å¤

### ä¿®å¤1ï¼šç§»é™¤ no_gradï¼ˆæœ€é‡è¦ï¼‰

åœ¨ `modeling/sdtps_complete.py` ä¸­ï¼š

```python
def _compute_self_attention(self, patches, global_feat):
    if global_feat.dim() == 2:
        global_feat = global_feat.unsqueeze(1)

    # ç§»é™¤ with torch.no_grad()
    patches_norm = F.normalize(patches, dim=-1)
    global_norm = F.normalize(global_feat, dim=-1)
    self_attn = (patches_norm * global_norm).sum(dim=-1)
    return self_attn
```

### ä¿®å¤2ï¼šæ›´æ–° test_sign

åœ¨ `engine/processor.py` line 47ï¼š

```python
test_sign = cfg.MODEL.HDM or cfg.MODEL.ATM or cfg.MODEL.USE_SDTPS
```

## ğŸ“Š å¯¹æ¯”å®éªŒå»ºè®®

1. **å¯¹ç…§ç»„**ï¼šè®­ç»ƒåŸå§‹ DeMoï¼ˆHDM+ATMï¼‰ï¼Œçœ‹æ”¶æ•›é€Ÿåº¦
2. **å®éªŒç»„1**ï¼šSDTPS + ç§»é™¤ no_grad
3. **å®éªŒç»„2**ï¼šSDTPS + æ›´å®½æ¾çš„å‹ç¼©æ¯”ä¾‹
4. **å®éªŒç»„3**ï¼šSDTPS + warm-up

## æ€»ç»“

**æœ€å¯èƒ½çš„åŸå› **ï¼š`with torch.no_grad()` é˜»æ–­äº† Backbone çš„å­¦ä¹ 

**ç«‹å³ä¿®å¤**ï¼šç§»é™¤ attention è®¡ç®—ä¸­çš„ `no_grad`

**åŸå› **ï¼š
- åŸ SEPS æ˜¯æ£€ç´¢ä»»åŠ¡ï¼ŒBackbone å†»ç»“
- æˆ‘ä»¬æ˜¯é‡è¯†åˆ«ä»»åŠ¡ï¼ŒBackbone éœ€è¦ finetune
- No_grad é˜»æ­¢äº† Backbone å­¦ä¹ æå–åˆ¤åˆ«æ€§ç‰¹å¾
