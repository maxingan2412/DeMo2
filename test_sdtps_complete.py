"""
æµ‹è¯•å®Œæ•´çš„ SDTPS æ¨¡å—
éªŒè¯: TokenSparse + TokenAggregation çš„å®Œæ•´æµç¨‹
"""

import torch
import sys
sys.path.insert(0, '.')

from modeling.sdtps_complete import TokenSparse, TokenAggregation, MultiModalSDTPS

def test_token_sparse():
    """æµ‹è¯• TokenSparse æ¨¡å—"""
    print("=" * 70)
    print("æµ‹è¯• TokenSparse æ¨¡å—")
    print("=" * 70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    batch = 4
    num_patches = 128
    feat_dim = 512
    sparse_ratio = 0.5

    # å‡†å¤‡æ•°æ®
    tokens = torch.randn(batch, num_patches, feat_dim).to(device)
    self_attn = torch.randn(batch, num_patches).to(device)
    cross_attn1 = torch.randn(batch, num_patches).to(device)
    cross_attn2 = torch.randn(batch, num_patches).to(device)

    # åˆ›å»ºæ¨¡å—
    sparse = TokenSparse(
        embed_dim=feat_dim,
        sparse_ratio=sparse_ratio,
        use_gumbel=False,
    ).to(device)

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        select_tokens, extra_token, score_mask, selected_mask, keep_indices = sparse(
            tokens=tokens,
            self_attention=self_attn,
            cross_attention_m2=cross_attn1,
            cross_attention_m3=cross_attn2,
            beta=0.25,
        )

    print(f"è¾“å…¥: tokens {tokens.shape}")
    print(f"è¾“å‡º: select_tokens {select_tokens.shape}")
    print(f"è¾“å‡º: extra_token {extra_token.shape}")
    print(f"è¾“å‡º: score_mask {score_mask.shape}")
    print(f"è¾“å‡º: selected_mask {selected_mask.shape}")
    print(f"è¾“å‡º: keep_indices {keep_indices.shape}")

    expected_n_s = int(num_patches * sparse_ratio)
    print(f"\né¢„æœŸé€‰ä¸­æ•°é‡: ceil({num_patches} Ã— {sparse_ratio}) = {expected_n_s}")
    print(f"å®é™…é€‰ä¸­æ•°é‡: {select_tokens.shape[1]}")
    print(f"å†³ç­–çŸ©é˜µä¸­1çš„æ•°é‡: {score_mask.sum(dim=1).float().mean().item():.1f}")

    assert select_tokens.shape == (batch, expected_n_s, feat_dim), "å½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸï¼"
    print("âœ“ TokenSparse æµ‹è¯•é€šè¿‡ï¼\n")


def test_token_aggregation():
    """æµ‹è¯• TokenAggregation æ¨¡å—"""
    print("=" * 70)
    print("æµ‹è¯• TokenAggregation æ¨¡å—")
    print("=" * 70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    batch = 4
    n_s = 64  # é€‰ä¸­çš„patches
    feat_dim = 512
    n_c = 26  # èšåˆåçš„patches

    # å‡†å¤‡æ•°æ®
    select_tokens = torch.randn(batch, n_s, feat_dim).to(device)

    # åˆ›å»ºæ¨¡å—
    aggr = TokenAggregation(
        dim=feat_dim,
        keeped_patches=n_c,
        dim_ratio=0.2,
    ).to(device)

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        aggr_tokens = aggr(select_tokens)

    print(f"è¾“å…¥: select_tokens {select_tokens.shape}")
    print(f"è¾“å‡º: aggr_tokens {aggr_tokens.shape}")
    print(f"\nèšåˆæ¯”ä¾‹: {n_c}/{n_s} = {n_c/n_s:.3f}")

    assert aggr_tokens.shape == (batch, n_c, feat_dim), "å½¢çŠ¶ä¸ç¬¦åˆé¢„æœŸï¼"
    print("âœ“ TokenAggregation æµ‹è¯•é€šè¿‡ï¼\n")


def test_multimodal_sdtps():
    """æµ‹è¯•å®Œæ•´çš„ MultiModalSDTPS"""
    print("=" * 70)
    print("æµ‹è¯• MultiModalSDTPS å®Œæ•´æµç¨‹")
    print("=" * 70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    batch = 4
    num_patches = 128
    feat_dim = 512
    sparse_ratio = 0.5
    aggr_ratio = 0.4

    # å‡†å¤‡æ•°æ®
    RGB_cash = torch.randn(batch, num_patches, feat_dim).to(device)
    NI_cash = torch.randn(batch, num_patches, feat_dim).to(device)
    TI_cash = torch.randn(batch, num_patches, feat_dim).to(device)
    RGB_global = torch.randn(batch, feat_dim).to(device)
    NI_global = torch.randn(batch, feat_dim).to(device)
    TI_global = torch.randn(batch, feat_dim).to(device)

    # åˆ›å»ºæ¨¡å—
    print()  # è¿™ä¼šæ‰“å°å‚æ•°ä¿¡æ¯
    sdtps = MultiModalSDTPS(
        embed_dim=feat_dim,
        num_patches=num_patches,
        sparse_ratio=sparse_ratio,
        aggr_ratio=aggr_ratio,
        use_gumbel=False,
    ).to(device)

    # è®¡ç®—å‚æ•°é‡
    n_params = sum(p.numel() for p in sdtps.parameters())
    print(f"  æ¨¡å—å‚æ•°é‡: {n_params / 1e6:.2f}M\n")

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        RGB_enh, NI_enh, TI_enh, rgb_mask, nir_mask, tir_mask = sdtps(
            RGB_cash, NI_cash, TI_cash,
            RGB_global, NI_global, TI_global
        )

    print(f"è¾“å…¥å½¢çŠ¶:")
    print(f"  RGB_cash: {RGB_cash.shape}")
    print(f"  NI_cash: {NI_cash.shape}")
    print(f"  TI_cash: {TI_cash.shape}")

    print(f"\nè¾“å‡ºå½¢çŠ¶:")
    print(f"  RGB_enhanced: {RGB_enh.shape}")
    print(f"  NI_enhanced: {NI_enh.shape}")
    print(f"  TI_enhanced: {TI_enh.shape}")

    print(f"\nå†³ç­–çŸ©é˜µ:")
    print(f"  rgb_mask: {rgb_mask.shape}, é€‰ä¸­: {rgb_mask.sum(dim=1).float().mean().item():.1f}")
    print(f"  nir_mask: {nir_mask.shape}, é€‰ä¸­: {nir_mask.sum(dim=1).float().mean().item():.1f}")
    print(f"  tir_mask: {tir_mask.shape}, é€‰ä¸­: {tir_mask.sum(dim=1).float().mean().item():.1f}")

    # éªŒè¯å½¢çŠ¶ä¸€è‡´æ€§
    assert RGB_enh.shape == NI_enh.shape == TI_enh.shape, "ä¸‰ä¸ªæ¨¡æ€å½¢çŠ¶ä¸ä¸€è‡´ï¼"

    expected_shape = (batch, int(num_patches * aggr_ratio * sparse_ratio) + 1, feat_dim)
    assert RGB_enh.shape == expected_shape, f"æœŸæœ› {expected_shape}ï¼Œå®é™… {RGB_enh.shape}"

    print(f"\nâœ“ å½¢çŠ¶éªŒè¯é€šè¿‡ï¼ä¸‰ä¸ªæ¨¡æ€è¾“å‡ºä¸€è‡´: {RGB_enh.shape}")
    print(f"âœ“ MultiModalSDTPS æµ‹è¯•é€šè¿‡ï¼\n")

    return sdtps


def test_complete_pipeline():
    """æµ‹è¯•å®Œæ•´çš„pipelineï¼ˆæ¨¡æ‹ŸçœŸå®ä½¿ç”¨ï¼‰"""
    print("=" * 70)
    print("æµ‹è¯•å®Œæ•´ Pipelineï¼ˆæ¨¡æ‹Ÿè®­ç»ƒï¼‰")
    print("=" * 70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åˆ›å»ºSDTPSæ¨¡å—
    sdtps = MultiModalSDTPS(
        embed_dim=512,
        num_patches=128,
        sparse_ratio=0.5,
        aggr_ratio=0.4,
        use_gumbel=True,  # æµ‹è¯•Gumbel
        gumbel_tau=1.0,
    ).to(device)

    print()
    sdtps.train()  # è®­ç»ƒæ¨¡å¼

    # å‡†å¤‡æ•°æ®
    batch = 4
    RGB_cash = torch.randn(batch, 128, 512).to(device)
    NI_cash = torch.randn(batch, 128, 512).to(device)
    TI_cash = torch.randn(batch, 128, 512).to(device)
    RGB_global = torch.randn(batch, 512).to(device)
    NI_global = torch.randn(batch, 512).to(device)
    TI_global = torch.randn(batch, 512).to(device)

    # å‰å‘ä¼ æ’­
    RGB_enh, NI_enh, TI_enh, rgb_mask, nir_mask, tir_mask = sdtps(
        RGB_cash, NI_cash, TI_cash,
        RGB_global, NI_global, TI_global
    )

    # æ± åŒ–å¾—åˆ°å…¨å±€ç‰¹å¾
    RGB_feat = RGB_enh.mean(dim=1)  # (B, N_c+1, C) â†’ (B, C)
    NI_feat = NI_enh.mean(dim=1)
    TI_feat = TI_enh.mean(dim=1)

    # æ‹¼æ¥
    final_feat = torch.cat([RGB_feat, NI_feat, TI_feat], dim=-1)  # (B, 3C)

    print(f"æœ€ç»ˆç‰¹å¾: {final_feat.shape}")
    print(f"âœ“ å®Œæ•´Pipelineæµ‹è¯•é€šè¿‡ï¼")

    # æµ‹è¯•æ¢¯åº¦
    print(f"\næµ‹è¯•æ¢¯åº¦åå‘ä¼ æ’­:")
    loss = final_feat.sum()
    loss.backward()

    # æ£€æŸ¥æ¢¯åº¦
    has_grad = any(p.grad is not None for p in sdtps.parameters())
    print(f"  å‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦: {has_grad}")

    if has_grad:
        grad_norm = sum(p.grad.norm().item() for p in sdtps.parameters() if p.grad is not None)
        print(f"  æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}")
        print(f"  âœ“ æ¢¯åº¦æ­£å¸¸ï¼")
    else:
        print(f"  âš ï¸  æ²¡æœ‰æ¢¯åº¦ï¼ˆå¯èƒ½æ˜¯no_gradå¯¼è‡´ï¼‰")


if __name__ == "__main__":
    print(f"ä½¿ç”¨è®¾å¤‡: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\n")

    try:
        # æµ‹è¯•å„ä¸ªç»„ä»¶
        test_token_sparse()
        test_token_aggregation()
        test_multimodal_sdtps()
        test_complete_pipeline()

        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼SDTPS å®Œæ•´å®ç°æ­£ç¡®ï¼")
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
